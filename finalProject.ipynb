{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CSE 546: Final Project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries Importation and Data Reading\n",
    "- Import the necessary libraries.\n",
    "- Load and inspect the movie reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import cm\n",
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    StratifiedKFold,\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import (\n",
    "    LabelEncoder,\n",
    "    MinMaxScaler,\n",
    "    StandardScaler,\n",
    "    RobustScaler,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    make_scorer,\n",
    ")\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest,\n",
    "    f_classif,\n",
    "    SelectFromModel,\n",
    "    VarianceThreshold,\n",
    ")\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    BaggingClassifier,\n",
    "    AdaBoostClassifier,\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import tree\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, ward, linkage\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"Data.csv\")\n",
    "df2 = pd.read_csv(\"extra_hard_samples.csv\")\n",
    "ds = pd.concat([df1, df2], axis=0).reset_index(drop=True)\n",
    "\n",
    "y = ds[\"class\"]\n",
    "images = ds[\"image_name\"]\n",
    "ds = ds.drop(\"class\", axis=1)\n",
    "ds = ds.drop(\"image_name\", axis=1)\n",
    "X=ds.to_numpy()\n",
    "y=y.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([(\"scaler\", StandardScaler()), (\"knn\", KNeighborsClassifier())])\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\"knn__n_neighbors\": range(1, 25)}\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"f1_score\": make_scorer(f1_score, average=\"weighted\"),\n",
    "    \"roc_auc\": make_scorer(roc_auc_score, multi_class=\"ovr\", needs_proba=True),\n",
    "}\n",
    "\n",
    "# Set up cross-validation\n",
    "cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring=scoring,\n",
    "    refit=\"accuracy\",\n",
    "    cv=cv,\n",
    "    verbose=3,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, np.array(y_train).ravel())\n",
    "\n",
    "# Output the results\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best mean cross-validation score: {:.4f}\".format(grid_search.best_score_))\n",
    "\n",
    "# Extract and plot results\n",
    "cv_results = grid_search.cv_results_\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "for idx, metric in enumerate(scoring.keys()):\n",
    "    plt.subplot(1, len(scoring), idx + 1)\n",
    "    metric_scores = cv_results[f\"mean_test_{metric}\"]\n",
    "    plt.plot(range(1, 25), metric_scores, marker=\"o\", label=f\"{metric.capitalize()}\")\n",
    "    plt.xlabel(\"Number of Neighbors (k)\")\n",
    "    plt.ylabel(f\"{metric.capitalize()} Score\")\n",
    "    plt.title(f\"{metric.capitalize()} vs. k\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Different Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Encode labels and preprocess features\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "X_change = pd.get_dummies(ds).to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_change, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"f1_score\": make_scorer(f1_score, average=\"weighted\"),\n",
    "    \"roc_auc\": make_scorer(roc_auc_score, multi_class=\"ovr\", needs_proba=True),\n",
    "}\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"knn\", KNeighborsClassifier(n_neighbors=21)),  # `n_neighbors` is fixed to 21\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define parameter grid for different distance metrics\n",
    "param_grid = {\"knn__metric\": [\"euclidean\", \"manhattan\", \"minkowski\"]}\n",
    "\n",
    "# Set up cross-validation\n",
    "cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring=scoring,\n",
    "    refit=\"accuracy\",\n",
    "    cv=cv,\n",
    "    verbose=3,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Output the results\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best mean cross-validation score: {:.4f}\".format(grid_search.best_score_))\n",
    "\n",
    "# Extract and display results for all metrics\n",
    "cv_results = grid_search.cv_results_\n",
    "\n",
    "for metric in scoring.keys():\n",
    "    best_idx = np.argmax(cv_results[f\"mean_test_{metric}\"])\n",
    "    best_score = cv_results[f\"mean_test_{metric}\"][best_idx]\n",
    "    print(\n",
    "        f\"Best {metric.capitalize()} Score: {best_score:.4f} (Metric: {cv_results['param_knn__metric'][best_idx]})\"\n",
    "    )\n",
    "\n",
    "# Plot the validation scores for each metric\n",
    "plt.figure(figsize=(12, 6))\n",
    "for idx, metric in enumerate(scoring.keys()):\n",
    "    plt.subplot(1, len(scoring), idx + 1)\n",
    "    metric_scores = [\n",
    "        cv_results[f\"mean_test_{metric}\"][i]\n",
    "        for i in range(len(cv_results[\"param_knn__metric\"]))\n",
    "    ]\n",
    "    plt.bar(cv_results[\"param_knn__metric\"], metric_scores, color=\"c\")\n",
    "    plt.xlabel(\"Distance Metric\")\n",
    "    plt.ylabel(f\"{metric.capitalize()} Score\")\n",
    "    plt.title(f\"{metric.capitalize()} Score by Metric\")\n",
    "    plt.ylim(0.0, 1.1)\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization - MinMax & Mean-Sigma & Median-MAD & RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class MADNormalization(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        # Compute the median and MAD for each feature\n",
    "        self.medians_ = np.median(X, axis=0)\n",
    "        self.mads_ = np.median(np.abs(X - self.medians_), axis=0)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Apply MAD normalization to each feature\n",
    "        return (X - self.medians_) / self.mads_\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "\n",
    "\n",
    "# Function to compute AUC-ROC\n",
    "def compute_aucroc(model, X_test, y_test):\n",
    "    y_probs = model.predict_proba(X_test)  # Get probabilities for AUC-ROC\n",
    "    aucroc = roc_auc_score(y_test, y_probs, multi_class=\"ovr\", average=\"weighted\")\n",
    "    return aucroc\n",
    "\n",
    "\n",
    "# Split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Preprocessing methods\n",
    "scalers = {\n",
    "    \"MinMax\": MinMaxScaler(),\n",
    "    \"Standard\": StandardScaler(),\n",
    "    \"Robust\": RobustScaler(),\n",
    "    \"MAD\": MADNormalization(),  # Add MAD normalization\n",
    "}\n",
    "\n",
    "# KNN Model Setup\n",
    "knn = KNeighborsClassifier(metric=\"euclidean\")\n",
    "\n",
    "# GridSearch setup with K values\n",
    "param_grid = {\"knn__n_neighbors\": np.arange(2, 34, 1)}\n",
    "\n",
    "# Pipeline setup\n",
    "pipeline = Pipeline(\n",
    "    [(\"scaler\", MinMaxScaler()), (\"knn\", knn)]  # Default to MinMax scaling initially\n",
    ")\n",
    "\n",
    "# Perform GridSearch with different scoring metrics\n",
    "scoring_metrics = [\n",
    "    \"accuracy\",\n",
    "    \"f1_macro\",\n",
    "    \"roc_auc_ovr\",\n",
    "]\n",
    "\n",
    "# For each scaling method\n",
    "for scaler_name, scaler in scalers.items():\n",
    "    print(f\"Running GridSearch for {scaler_name} Scaling...\")\n",
    "\n",
    "    # Update pipeline with the current scaler\n",
    "    pipeline.set_params(scaler=scaler)\n",
    "\n",
    "    # GridSearchCV with multiple scoring metrics\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, param_grid, scoring=scoring_metrics, refit=\"roc_auc_ovr\", cv=5\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best results\n",
    "    best_params = grid_search.best_params_\n",
    "    best_scores = grid_search.cv_results_\n",
    "\n",
    "    # Output the best parameters and scores for each metric\n",
    "    print(f\"Best parameters for {scaler_name} scaling: {best_params}\")\n",
    "    for metric in scoring_metrics:\n",
    "        print(\n",
    "            f\"Best {metric.capitalize()}: {best_scores['mean_test_' + metric][best_scores['rank_test_' + metric] == 1][0]:.4f}\"\n",
    "        )\n",
    "\n",
    "    # Plot accuracy vs k-value (KNN neighbors) for the best model\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.plot(\n",
    "        best_scores[\"param_knn__n_neighbors\"],\n",
    "        best_scores[\"mean_test_accuracy\"],\n",
    "        marker=\"o\",\n",
    "        linestyle=\"-\",\n",
    "        label=\"Accuracy\",\n",
    "    )\n",
    "    ax.set_xlabel(\"K-Value\", fontsize=20)\n",
    "    ax.set_ylabel(\"Accuracy\", fontsize=20)\n",
    "    ax.set_title(f\"{scaler_name} Scaling: K-Value vs Accuracy\", fontsize=22)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot AUC-ROC vs k-value for the best model\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.plot(\n",
    "        best_scores[\"param_knn__n_neighbors\"],\n",
    "        best_scores[\"mean_test_roc_auc_ovr\"],\n",
    "        marker=\"o\",\n",
    "        linestyle=\"-\",\n",
    "        label=\"AUC-ROC\",\n",
    "    )\n",
    "    ax.set_xlabel(\"K-Value\", fontsize=20)\n",
    "    ax.set_ylabel(\"AUC-ROC\", fontsize=20)\n",
    "    ax.set_title(f\"{scaler_name} Scaling: K-Value vs AUC-ROC\", fontsize=22)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Feature selection #1\n",
    "fig,ax=plt.subplots(figsize=(10,10))\n",
    "k_list=np.arange(55,120,1)\n",
    "knn_dict={}\n",
    "for i in k_list:\n",
    "    selector = SelectKBest(score_func=f_classif, k=i)\n",
    "    X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "    print(i)\n",
    "    knn=KNeighborsClassifier(n_neighbors=21, metric='euclidean')\n",
    "    model_knn=knn.fit(X_train,y_train.ravel())\n",
    "    y_knn_pred=model_knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_knn_pred)\n",
    "    knn_dict[i] = accuracy\n",
    "best_k = max(knn_dict, key=knn_dict.get)\n",
    "best_accuracy = knn_dict[best_k]\n",
    "print(f\"Feature Select #1: Highest Accuracy: {best_accuracy:.4f} at k={best_k}\")\n",
    "ax.plot(knn_dict.keys(),knn_dict.values(), marker='o', linestyle='-')\n",
    "ax.set_xlabel('K-VALUE', fontsize=20)\n",
    "ax.set_ylabel('Accuracy',fontsize=20)\n",
    "ax.set_title('K-Value vs Accuracy', fontsize=22)\n",
    "plt.show()\n",
    "\n",
    "# KNN Feature selection #2 -  model based fs  ------------\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X, y)\n",
    "selector = SelectFromModel(rf, threshold=\"median\") #selects above the median\n",
    "selector.fit(X, y)\n",
    "X_selected = selector.transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Feature Selection #2: Accuracy with RF-selected features: {accuracy:.4f}\")\n",
    "selected_features = selector.get_support(indices=True)\n",
    "print(f\"Feature Selection #2: Selected feature indices: {selected_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Feature selection #3\n",
    "\n",
    "selector = VarianceThreshold(threshold=0)  # change threshold as needed\n",
    "\n",
    "X_train_selected = selector.fit_transform(X_train)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_selected, y_train)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "fig,ax=plt.subplots(figsize=(10,10))\n",
    "k_list=np.arange(2,34,1)\n",
    "knn_dict={}\n",
    "for i in k_list:\n",
    "    print(i)\n",
    "    knn=KNeighborsClassifier(n_neighbors=int(i), metric='euclidean')\n",
    "    model_knn=knn.fit(X_train_selected,y_train.ravel())\n",
    "    y_knn_pred=model_knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_knn_pred)\n",
    "    knn_dict[i] = accuracy\n",
    "best_k = max(knn_dict, key=knn_dict.get)\n",
    "best_accuracy = knn_dict[best_k]\n",
    "print(f\"No preprocessing: Highest Accuracy: {best_accuracy:.4f} at k={best_k}\")\n",
    "ax.plot(knn_dict.keys(),knn_dict.values(), marker='o', linestyle='-')\n",
    "ax.set_xlabel('K-VALUE', fontsize=20)\n",
    "ax.set_ylabel('Accuracy',fontsize=20)\n",
    "ax.set_title('K-Value vs Accuracy', fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNB Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# MinMaxScaler inside a pipeline\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Define the hyperparameter grid for Multinomial Naive Bayes\n",
    "param_grid = {\n",
    "    \"mnb__alpha\": np.logspace(-4, 4, 9)  # Alpha values for MNB, range from 1e-4 to 1e4\n",
    "}\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"roc_auc\": make_scorer(\n",
    "        roc_auc_score, multi_class=\"ovo\"\n",
    "    ),  # AUC-ROC for multi-class classification\n",
    "    \"f1\": make_scorer(f1_score, average=\"weighted\"),\n",
    "}\n",
    "\n",
    "# Set up Stratified K-Fold cross-validation\n",
    "cv = KFold(n_splits=4, shuffle=True, random_state=50)\n",
    "\n",
    "# Create the pipeline with MinMaxScaler and Multinomial Naive Bayes\n",
    "pipeline = Pipeline(\n",
    "    [(\"scaler\", scaler), (\"mnb\", MultinomialNB())]  # Scaling step  # MNB classifier\n",
    ")\n",
    "\n",
    "# GridSearchCV for MNB with multiple scoring metrics\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring=scoring,\n",
    "    refit=\"accuracy\",  # Use accuracy for model selection\n",
    "    cv=cv,\n",
    "    return_train_score=True,\n",
    "    verbose=3,\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract results into a DataFrame\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Display top results sorted by validation accuracy\n",
    "top_results = results_df.sort_values(by=\"mean_test_accuracy\", ascending=False).head(5)\n",
    "print(\"Top 5 Hyperparameter Combinations:\")\n",
    "print(\n",
    "    top_results[\n",
    "        [\"param_mnb__alpha\", \"mean_test_accuracy\", \"mean_test_f1\", \"mean_test_roc_auc\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Best parameters and score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"\\nBest Parameters: {best_params}\")\n",
    "print(f\"Best Validation Accuracy: {best_score:.4f}\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_accuracy = grid_search.score(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Visualization of metrics for the best alpha\n",
    "metrics = [\n",
    "    \"mean_test_accuracy\",\n",
    "    \"mean_test_f1\",\n",
    "    \"mean_test_roc_auc\",  # Include AUC-ROC in the visualization\n",
    "]\n",
    "# Extracting the metric values for the best alpha value\n",
    "best_alpha_values = results_df[\"param_mnb__alpha\"].values\n",
    "metrics_values = results_df[metrics].values\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Iterate over each metric and plot it separately\n",
    "for idx, metric in enumerate(metrics):\n",
    "    plt.plot(\n",
    "        best_alpha_values,\n",
    "        metrics_values[:, idx],\n",
    "        marker=\"o\",\n",
    "        linestyle=\"-\",\n",
    "        label=metric,\n",
    "    )\n",
    "\n",
    "plt.title(\"Performance Metrics for Best Hyperparameters\", fontsize=14)\n",
    "plt.ylabel(\"Score\", fontsize=12)\n",
    "plt.xlabel(\"Alpha (Regularization Strength)\", fontsize=12)\n",
    "plt.xscale(\"log\")  # Log scale for alpha\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNB Data normalization #1: MinMax\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled1 = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled1, y, test_size=0.2, random_state=42)\n",
    "neighbors = np.arange(1, 20) \n",
    "flag = .00001\n",
    "train_accuracy = np.empty(len(neighbors)) \n",
    "test_accuracy = np.empty(len(neighbors)) \n",
    "alpha_values = []\n",
    "for i in range(0, 19): \n",
    "    flag *= 10\n",
    "    mnb = MultinomialNB(alpha=flag, fit_prior=True)\n",
    "    mnb.fit(X_train, y_train)\n",
    "    train_accuracy[i] = mnb.score(X_train, y_train) \n",
    "    test_accuracy[i] = mnb.score(X_test, y_test) \n",
    "    alpha_values.append(flag)\n",
    "best_index = np.argmax(test_accuracy)\n",
    "best_alpha = alpha_values[best_index]\n",
    "best_test_acc = test_accuracy[best_index]\n",
    "print(f\"MinMax: Highest Test Accuracy: {best_test_acc:.4f} at Alpha = {best_alpha:.4e}\")\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy') \n",
    "plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy') \n",
    "plt.legend() \n",
    "plt.xlabel('run time starting with .0001 then times 10 each run') \n",
    "plt.ylabel('Accuracy') \n",
    "plt.show()\n",
    "\n",
    "# MNB Data normalization #2: Mean-Sigma\n",
    "scaler = StandardScaler()\n",
    "X_scaled2 = scaler.fit_transform(X)\n",
    "if (X_scaled2 < 0).any():\n",
    "    X_scaled2 -= X_scaled2.min(axis=0)  #make all features non-negative\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled2, y, test_size=0.2, random_state=42)\n",
    "neighbors = np.arange(1, 20) \n",
    "flag = .00001\n",
    "train_accuracy = np.empty(len(neighbors)) \n",
    "test_accuracy = np.empty(len(neighbors)) \n",
    "alpha_values = []\n",
    "for i in range(0, 19): \n",
    "    flag *= 10\n",
    "    mnb = MultinomialNB(alpha=flag, fit_prior=True)\n",
    "    mnb.fit(X_train, y_train)\n",
    "    train_accuracy[i] = mnb.score(X_train, y_train) \n",
    "    test_accuracy[i] = mnb.score(X_test, y_test) \n",
    "    alpha_values.append(flag)\n",
    "best_index = np.argmax(test_accuracy)\n",
    "best_alpha = alpha_values[best_index]\n",
    "best_test_acc = test_accuracy[best_index]\n",
    "print(f\"Mean-Sigma: Highest Test Accuracy: {best_test_acc:.4f} at Alpha = {best_alpha:.4e}\")\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy') \n",
    "plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy') \n",
    "plt.legend() \n",
    "plt.xlabel('run time starting with .0001 then times 10 each run') \n",
    "plt.ylabel('Accuracy') \n",
    "plt.show()\n",
    "\n",
    "# MNB Data normalization #3: Median-MAD\n",
    "def median_mad_normalize_nonnegative(data):\n",
    "    medians = np.median(data, axis=0)\n",
    "    mad = np.median(np.abs(data - medians), axis=0)\n",
    "    mad[mad == 0] = 1  # Prevent division by zero\n",
    "    normalized_data = (data - medians) / mad\n",
    "\n",
    "    # Shift to make all values non-negative\n",
    "    min_val = np.min(normalized_data)\n",
    "    if min_val < 0:\n",
    "        normalized_data += abs(min_val) + 1e-6  # Shift to ensure non-negativity\n",
    "    return normalized_data\n",
    "X_scaled3 = median_mad_normalize_nonnegative(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled3, y, test_size=0.2, random_state=42)\n",
    "neighbors = np.arange(1, 20) \n",
    "flag = .00001\n",
    "train_accuracy = np.empty(len(neighbors)) \n",
    "test_accuracy = np.empty(len(neighbors)) \n",
    "alpha_values = []\n",
    "for i in range(0, 19): \n",
    "    flag *= 10\n",
    "    mnb = MultinomialNB(alpha=flag, fit_prior=True)\n",
    "    mnb.fit(X_train, y_train)\n",
    "    train_accuracy[i] = mnb.score(X_train, y_train) \n",
    "    test_accuracy[i] = mnb.score(X_test, y_test) \n",
    "    alpha_values.append(flag)\n",
    "best_index = np.argmax(test_accuracy)\n",
    "best_alpha = alpha_values[best_index]\n",
    "best_test_acc = test_accuracy[best_index]\n",
    "print(f\"Median-MAD: Highest Test Accuracy: {best_test_acc:.4f} at Alpha = {best_alpha:.4e}\")\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy') \n",
    "plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy') \n",
    "plt.legend() \n",
    "plt.xlabel('run time starting with .0001 then times 10 each run') \n",
    "plt.ylabel('Accuracy') \n",
    "plt.show()\n",
    "\n",
    "# MNB Data normalization #4: robustScaler\n",
    "robust_scaler = RobustScaler()\n",
    "X_scaled4 = robust_scaler.fit_transform(X)\n",
    "X_scaled4 = X_scaled4 - X_scaled4.min(axis=0) #shift so no negatives\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled4, y, test_size=0.2, random_state=42)\n",
    "neighbors = np.arange(1, 20) \n",
    "flag = .00001\n",
    "train_accuracy = np.empty(len(neighbors)) \n",
    "test_accuracy = np.empty(len(neighbors)) \n",
    "alpha_values = []\n",
    "for i in range(0, 19): \n",
    "    flag *= 10\n",
    "    mnb = MultinomialNB(alpha=flag, fit_prior=True)\n",
    "    mnb.fit(X_train, y_train)\n",
    "    train_accuracy[i] = mnb.score(X_train, y_train) \n",
    "    test_accuracy[i] = mnb.score(X_test, y_test) \n",
    "    alpha_values.append(flag)\n",
    "best_index = np.argmax(test_accuracy)\n",
    "best_alpha = alpha_values[best_index]\n",
    "best_test_acc = test_accuracy[best_index]\n",
    "print(f\"robust scaler: Highest Test Accuracy: {best_test_acc:.4f} at Alpha = {best_alpha:.4e}\")\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy') \n",
    "plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy') \n",
    "plt.legend() \n",
    "plt.xlabel('run time starting with .0001 then times 10 each run') \n",
    "plt.ylabel('Accuracy') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNB Feature selection #1\n",
    "scaler = MinMaxScaler()\n",
    "X_scaleMNBMinMax = scaler.fit_transform(X)\n",
    "k = 50  # Number of top features to select\n",
    "selector = SelectKBest(score_func=f_classif, k=k)\n",
    "X_selectMNBagain = selector.fit_transform(X_scaleMNBMinMax, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selectMNBagain, y, test_size=0.2, random_state=42)\n",
    "neighbors = np.arange(1, 20)\n",
    "flag = .00001\n",
    "train_accuracy = np.empty(len(neighbors))\n",
    "test_accuracy = np.empty(len(neighbors))\n",
    "alpha_values = []\n",
    "for i in range(0, 19):\n",
    "    flag *= 10\n",
    "    mnb = MultinomialNB(alpha=flag, fit_prior=True)\n",
    "    mnb.fit(X_train, y_train)\n",
    "    train_accuracy[i] = mnb.score(X_train, y_train)\n",
    "    test_accuracy[i] = mnb.score(X_test, y_test)\n",
    "    alpha_values.append(flag)\n",
    "best_index = np.argmax(test_accuracy)\n",
    "best_alpha = alpha_values[best_index]\n",
    "best_test_acc = test_accuracy[best_index]\n",
    "print(f\"MNB Feature selection 1: Highest Test Accuracy: {best_test_acc:.4f} at Alpha = {best_alpha:.4e}\")\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy')\n",
    "plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('run time starting with .0001 then times 10 each run')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# MNB Feature selection #2 - another method found online that uses RFC\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_scaleMNBMinMax, y)\n",
    "selector = SelectFromModel(rf, threshold=\"median\") #selects above the median\n",
    "selector.fit(X_scaleMNBMinMax, y)\n",
    "X_selected = selector.transform(X_scaleMNBMinMax)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "mnb = MultinomialNB(alpha=100, fit_prior=True) #select optimal value\n",
    "mnb.fit(X_train, y_train)\n",
    "y_pred = mnb.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy with RF-selected features: {accuracy:.4f}\")\n",
    "selected_features = selector.get_support(indices=True)\n",
    "print(f\"Selected feature indices: {selected_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNB Feature selection #3\n",
    "selector = VarianceThreshold(threshold=0.01)  # change threshold as needed\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled1 = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled1, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_selected = selector.fit_transform(X_train)\n",
    "\n",
    "neighbors = np.arange(1, 20)\n",
    "flag = .00001\n",
    "train_accuracy = np.empty(len(neighbors))\n",
    "test_accuracy = np.empty(len(neighbors))\n",
    "alpha_values = []\n",
    "for i in range(0, 19):\n",
    "    flag *= 10\n",
    "    mnb = MultinomialNB(alpha=flag, fit_prior=True)\n",
    "    mnb.fit(X_train, y_train)\n",
    "    train_accuracy[i] = mnb.score(X_train_selected, y_train)\n",
    "    test_accuracy[i] = mnb.score(X_test, y_test)\n",
    "    alpha_values.append(flag)\n",
    "best_index = np.argmax(test_accuracy)\n",
    "best_alpha = alpha_values[best_index]\n",
    "best_test_acc = test_accuracy[best_index]\n",
    "print(f\"MNB Feature selection 1: Highest Test Accuracy: {best_test_acc:.4f} at Alpha = {best_alpha:.4e}\")\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy')\n",
    "plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('run time starting with .0001 then times 10 each run')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels if they are not already numeric\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)  # Encode the labels\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define the hyperparameter grid for Random Forest\n",
    "param_grid = {\n",
    "    \"max_depth\": [None, 5, 10, 20, 30, 50, 100],  # Max depth values to tune\n",
    "    \"n_estimators\": [1000],  # Number of trees\n",
    "}\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"roc_auc\": make_scorer(\n",
    "        roc_auc_score,average=\"macro\", multi_class=\"ovr\",\n",
    "    ),  # AUC-ROC for multi-class classification\n",
    "    \"f1\": make_scorer(f1_score, average=\"weighted\"),\n",
    "}\n",
    "\n",
    "# Set up Stratified K-Fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=50)\n",
    "\n",
    "# GridSearchCV for RandomForest with multiple scoring metrics\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    scoring=scoring,\n",
    "    refit=\"accuracy\",  # Use accuracy for model selection\n",
    "    cv=cv,\n",
    "    return_train_score=True,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract results into a DataFrame\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Display top results sorted by validation accuracy\n",
    "top_results = results_df.sort_values(by=\"mean_test_accuracy\", ascending=False).head(5)\n",
    "print(\"Top 5 Hyperparameter Combinations:\")\n",
    "print(\n",
    "    top_results[\n",
    "        [\"param_max_depth\", \"mean_test_accuracy\", \"mean_test_f1\", \"mean_test_roc_auc\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Best parameters and score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"\\nBest Parameters: {best_params}\")\n",
    "print(f\"Best Validation Accuracy: {best_score:.4f}\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_accuracy = grid_search.score(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Visualization of metrics for the best hyperparameters\n",
    "metrics = [\n",
    "    \"mean_test_accuracy\",\n",
    "    \"mean_test_f1\",\n",
    "    \"mean_test_roc_auc\",  # Include AUC-ROC in the visualization\n",
    "]\n",
    "best_results = results_df[(results_df[\"param_max_depth\"] == best_params[\"max_depth\"])]\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.plot(\n",
    "    best_results[\"param_max_depth\"],  # x-values: multiple max_depth values\n",
    "    best_results[metrics].values.flatten(),  # y-values: corresponding metric values\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    color=\"blue\",\n",
    "    label=\"Metrics Scores\",\n",
    ")\n",
    "plt.title(\"Performance Metrics for Best Hyperparameters\", fontsize=14)\n",
    "plt.ylabel(\"Score\", fontsize=12)\n",
    "plt.xlabel(\"Max Depth\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scoring metrics\n",
    "scoring_metrics = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"roc_auc\": make_scorer(roc_auc_score, multi_class=\"ovr\"),\n",
    "    \"f1\": make_scorer(f1_score, average=\"weighted\"),\n",
    "}\n",
    "\n",
    "# Prepare data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define the RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=5, random_state=42)\n",
    "\n",
    "# Define preprocessors for different normalization techniques\n",
    "scalers = {\n",
    "    \"minmax\": MinMaxScaler(),\n",
    "    \"standard\": StandardScaler(),\n",
    "    \"robust\": RobustScaler(),\n",
    "}\n",
    "\n",
    "# Grid search parameters\n",
    "param_grid = {\n",
    "    \"rf__max_depth\": [None, 5, 10, 20, 30, 50, 100],\n",
    "    \"rf__n_estimators\": [5, 10, 50, 100],\n",
    "}\n",
    "\n",
    "# Prepare a figure to plot all results\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Iterate over scalers\n",
    "for scaler_name, scaler in scalers.items():\n",
    "    # Create a pipeline that includes preprocessing and the classifier\n",
    "    pipeline = Pipeline([(\"scaler\", scaler), (\"rf\", rf)])\n",
    "\n",
    "    # Initialize GridSearchCV with the defined parameters and scoring metrics\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        scoring=scoring_metrics,\n",
    "        refit=\"accuracy\",\n",
    "        cv=5,\n",
    "        return_train_score=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # Fit the model with GridSearch\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best model from GridSearch\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Print the best parameters and scores\n",
    "    print(f\"Scaler: {scaler_name}\")\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best Accuracy: {grid_search.best_score_:.4f}\")\n",
    "    print(\n",
    "        f\"Best test Accuracy: {accuracy_score(y_test, best_model.predict(X_test)):.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Best test ROC AUC: {roc_auc_score(y_test, best_model.predict_proba(X_test), multi_class='ovr'):.4f}\"\n",
    "    )\n",
    "\n",
    "    # Plot the results for each scoring metric\n",
    "    results = grid_search.cv_results_\n",
    "\n",
    "    for score_name, score_func in scoring_metrics.items():\n",
    "        plt.plot(\n",
    "            results[\"param_rf__max_depth\"],\n",
    "            results[f\"mean_test_{score_name}\"],\n",
    "            label=f\"{score_name} - {scaler_name}\",\n",
    "        )\n",
    "\n",
    "# Customizing the plot\n",
    "plt.title(\"GridSearchCV Results for Different Scalers and Metrics\")\n",
    "plt.xlabel(\"Max Depth\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection, Feature Importance and Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF Feature selection #1\n",
    "scaler = MinMaxScaler()\n",
    "X_MinMaxRF = scaler.fit_transform(X)\n",
    "k = 50  # Number of top features to select\n",
    "selector = SelectKBest(score_func=f_classif, k=k)\n",
    "X_RFSelector = selector.fit_transform(X_MinMaxRF, y)\n",
    "numberOfNEstimatorsRF = 1000\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_RFSelector, y, test_size=0.2, random_state=42)\n",
    "max_depth_values = [None,5, 10, 20, 30, 50, 100]\n",
    "neighbors = np.arange(1, 8)\n",
    "train_accuracy = np.empty(len(neighbors))\n",
    "test_accuracy = np.empty(len(neighbors))\n",
    "depth_values = []\n",
    "for i in range(0, 7):\n",
    "    print(max_depth_values[i])\n",
    "    forest = RandomForestClassifier(n_estimators=numberOfNEstimatorsRF,max_depth=max_depth_values[i], random_state=42)\n",
    "    forest.fit(X_train, y_train)\n",
    "    train_accuracy[i] = forest.score(X_train, y_train)\n",
    "    test_accuracy[i] = forest.score(X_test, y_test)\n",
    "    depth_values.append(max_depth_values[i])\n",
    "best_index = np.argmax(test_accuracy)\n",
    "best_alpha = depth_values[best_index]\n",
    "best_test_acc = test_accuracy[best_index]\n",
    "print(f\"Highest Test Accuracy: {best_test_acc:.4f} at depth = {best_alpha}\")\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy')\n",
    "plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('[None,5, 10, 20, 30, 50, 100] max_depth values')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# RF Feature importance graph #1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "lengthOfFeaturesInOrder = np.arange(1, 257)\n",
    "forest = RandomForestClassifier(n_estimators=numberOfNEstimatorsRF, random_state=0)\n",
    "forest.fit(X_train, y_train.ravel())\n",
    "importances = forest.feature_importances_\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.barh(lengthOfFeaturesInOrder, importances, color='skyblue')\n",
    "plt.xlabel('Random Forest Importance')\n",
    "plt.title('Feature Importance - RF Importance')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis for better visualization\n",
    "plt.show()\n",
    "\n",
    "#RF Feature importance graph #2\n",
    "RF = RandomForestClassifier(n_estimators=numberOfNEstimatorsRF, max_depth=None, random_state=0)\n",
    "RF.fit(X_train, y_train.ravel())\n",
    "RF_feature_weights = RF.feature_importances_\n",
    "feature_weights=RF_feature_weights.flatten()\n",
    "# Create a DataFrame to store the features and their corresponding weights\n",
    "features_df = pd.DataFrame({\n",
    "    'Feature': ds.columns,\n",
    "    'Weight': feature_weights\n",
    "})\n",
    "# Sort by the absolute value of weights to see the most important features\n",
    "features_df['Absolute Weight'] = np.abs(features_df['Weight'])\n",
    "features_df = features_df.sort_values(by='Absolute Weight', ascending=False)\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"Feature\", y=\"Weight\", data=features_df.head(20), palette=\"coolwarm\")\n",
    "plt.title(\"Top 10 Features by Weight in RF Model\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Weight\")\n",
    "# Rotate the x labels by 45 degrees\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plotting the RF trees(change n_estimators to 1000 for actual results)\n",
    "RF = RandomForestClassifier(n_estimators=numberOfNEstimatorsRF, max_depth=5, random_state=0)\n",
    "RF.fit(X_train, y_train.ravel())\n",
    "plt.figure(figsize=(12, 5))\n",
    "tree.plot_tree(RF.estimators_[0], filled=True)\n",
    "plt.title(\"First Tree in Random Forest\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the last tree\n",
    "plt.figure(figsize=(12, 5))\n",
    "tree.plot_tree(RF.estimators_[-1], filled=True)\n",
    "plt.title(\"Last Tree in Random Forest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF Feature selection #3\n",
    "selector = VarianceThreshold(threshold=0.01)  # change threshold as needed\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_selected = selector.fit_transform(X_train)\n",
    "\n",
    "max_depth_values = [None,5, 10, 20, 30, 50, 100]\n",
    "neighbors = np.arange(1, 8)\n",
    "train_accuracy = np.empty(len(neighbors))\n",
    "test_accuracy = np.empty(len(neighbors))\n",
    "depth_values = []\n",
    "for i in range(0, 7):\n",
    "    print(max_depth_values[i])\n",
    "    forest = RandomForestClassifier(n_estimators=1000,max_depth=max_depth_values[i], random_state=42)\n",
    "    forest.fit(X_train, y_train)\n",
    "    train_accuracy[i] = forest.score(X_train, y_train)\n",
    "    test_accuracy[i] = forest.score(X_test, y_test)\n",
    "    depth_values.append(max_depth_values[i])\n",
    "best_index = np.argmax(test_accuracy)\n",
    "best_alpha = depth_values[best_index]\n",
    "best_test_acc = test_accuracy[best_index]\n",
    "print(f\"Highest Test Accuracy: {best_test_acc:.4f} at depth = {best_alpha}\")\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy')\n",
    "plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('[None,5, 10, 20, 30, 50, 100] max_depth values')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GB Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),  # Optional: scale features\n",
    "        (\"classifier\", GradientBoostingClassifier(random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    \"classifier__n_estimators\": [1, 5, 10, 20, 50, 100, 200, 500],\n",
    "    \"classifier__learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    \"classifier__max_depth\": [3],  # Fix max_depth for simplicity\n",
    "}\n",
    "\n",
    "# Define multiple scoring metrics, including AUC-ROC\n",
    "scoring = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"roc_auc\": make_scorer(\n",
    "        roc_auc_score, multi_class=\"ovr\"\n",
    "    ),  # AUC-ROC for multi-class classification\n",
    "    \"f1\": make_scorer(f1_score, average=\"weighted\"),\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV with multiple scoring\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring=scoring,\n",
    "    refit=\"accuracy\",  # Use 'accuracy' as the primary metric to select the best model\n",
    "    cv=4,  # 4-fold cross-validation\n",
    "    verbose=3,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the results\n",
    "results = grid_search.cv_results_\n",
    "\n",
    "# Best parameters and score\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Visualize results for accuracy vs. n_estimators\n",
    "n_estimators = [\n",
    "    param[\"classifier__n_estimators\"] for param in grid_search.cv_results_[\"params\"]\n",
    "]\n",
    "mean_test_accuracy = results[\"mean_test_accuracy\"]\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.plot(\n",
    "    n_estimators,\n",
    "    mean_test_accuracy,\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    color=\"blue\",\n",
    "    label=\"Validation Accuracy\",\n",
    ")\n",
    "plt.xscale(\"log\")  # Use a logarithmic scale for n_estimators\n",
    "plt.xlabel(\"Number of Estimators (n_estimators)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Validation Accuracy vs. n_estimators\")\n",
    "plt.ylim(0.1, 1.1)  # Set y-axis limits\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Access scores for all metrics\n",
    "print(\"Detailed GridSearch Results:\")\n",
    "for metric in scoring.keys():\n",
    "    mean_scores = results[f\"mean_test_{metric}\"]\n",
    "    print(f\"\\nMetric: {metric}\")\n",
    "    for param, score in zip(results[\"params\"], mean_scores):\n",
    "        print(f\"Params: {param}, {metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median-MAD normalization function\n",
    "def median_mad_normalize(X):\n",
    "    medians = np.median(X, axis=0)\n",
    "    mad = np.median(np.abs(X - medians), axis=0)\n",
    "    mad[mad == 0] = 1e-6  # Avoid division by zero\n",
    "    return (X - medians) / mad\n",
    "\n",
    "\n",
    "# Create scorers including AUC-ROC\n",
    "scorers = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"roc_auc\": make_scorer(\n",
    "        roc_auc_score, multi_class=\"ovr\"\n",
    "    ),  # AUC-ROC for multi-class classification\n",
    "    \"f1\": make_scorer(f1_score, average=\"weighted\"),\n",
    "}\n",
    "\n",
    "# Data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# n_estimators range\n",
    "n_estimators_range = [1, 5, 10, 20, 50, 100, 200, 500]\n",
    "\n",
    "# List of preprocessing techniques\n",
    "scalers = {\n",
    "    \"No Preprocessing\": None,\n",
    "    \"MinMaxScaler\": MinMaxScaler(),\n",
    "    \"StandardScaler\": StandardScaler(),\n",
    "    \"Median-MAD\": \"Median-MAD\",\n",
    "    \"RobustScaler\": RobustScaler(),\n",
    "}\n",
    "\n",
    "# To store results\n",
    "results = {}\n",
    "\n",
    "# Loop through each scaler\n",
    "for scaler_name, scaler in scalers.items():\n",
    "    print(f\"Processing: {scaler_name}\")\n",
    "\n",
    "    # Apply Median-MAD manually if needed\n",
    "    if scaler_name == \"Median-MAD\":\n",
    "        X_train_scaled = median_mad_normalize(X_train)\n",
    "        X_test_scaled = median_mad_normalize(X_test)\n",
    "    elif scaler:  # Use other scalers\n",
    "        scaler.fit(X_train)\n",
    "        X_train_scaled = scaler.transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    else:  # No preprocessing\n",
    "        X_train_scaled = X_train\n",
    "        X_test_scaled = X_test\n",
    "\n",
    "    # GridSearch for GBC\n",
    "    gbc = GradientBoostingClassifier(max_depth=3, learning_rate=0.1, random_state=42)\n",
    "    param_grid = {\"n_estimators\": n_estimators_range}\n",
    "    grid_search = GridSearchCV(\n",
    "        gbc, param_grid, scoring=scorers, cv=5, n_jobs=-1, refit=\"accuracy\", verbose=3\n",
    "    )\n",
    "    grid_search.fit(X_train_scaled, y_train.ravel())\n",
    "\n",
    "    # Best parameters and accuracy\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    print(\n",
    "        f\"{scaler_name}: Best n_estimators = {best_params['n_estimators']}, Best Accuracy (CV) = {best_score:.4f}\"\n",
    "    )\n",
    "\n",
    "    # Test set scores\n",
    "    best_gbc = grid_search.best_estimator_\n",
    "    test_scores = {\n",
    "        scorer: grid_search.scorer_[scorer](best_gbc, X_test_scaled, y_test)\n",
    "        for scorer in scorers\n",
    "    }\n",
    "    print(f\"{scaler_name}: Test Scores: {test_scores}\")\n",
    "\n",
    "    # Save results\n",
    "    results[scaler_name] = {\n",
    "        \"Best Params\": best_params,\n",
    "        \"CV Scores\": grid_search.cv_results_,\n",
    "        \"Test Scores\": test_scores,\n",
    "    }\n",
    "\n",
    "    # Plot train/test accuracy over n_estimators\n",
    "    train_accuracy = []\n",
    "    test_accuracy = []\n",
    "    for n in n_estimators_range:\n",
    "        gbc = GradientBoostingClassifier(\n",
    "            n_estimators=n, max_depth=3, learning_rate=0.1, random_state=42\n",
    "        )\n",
    "        gbc.fit(X_train_scaled, y_train.ravel())\n",
    "        train_accuracy.append(gbc.score(X_train_scaled, y_train))\n",
    "        test_accuracy.append(gbc.score(X_test_scaled, y_test))\n",
    "\n",
    "    plt.plot(n_estimators_range, test_accuracy, label=\"Testing Accuracy\", marker=\"o\")\n",
    "    plt.plot(n_estimators_range, train_accuracy, label=\"Training Accuracy\", marker=\"o\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"n_estimators\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(f\"GBC Accuracy with {scaler_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Final Results\n",
    "print(\"\\nSummary of Results:\")\n",
    "for scaler_name, result in results.items():\n",
    "    print(f\"{scaler_name}:\")\n",
    "    print(f\"  Best Params: {result['Best Params']}\")\n",
    "    print(f\"  Test Scores: {result['Test Scores']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection, Feature Importance and Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBC Feature selection #1\n",
    "scaler = MinMaxScaler()\n",
    "X_scalerMinMaxGBC = scaler.fit_transform(X)\n",
    "k = 50  # Number of top features to select\n",
    "selector = SelectKBest(score_func=f_classif, k=k)\n",
    "XSelectorGBC = selector.fit_transform(X_scalerMinMaxGBC, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(XSelectorGBC, y, test_size=0.2, random_state=42)\n",
    "n_range = [1, 5, 10, 20, 50, 100, 200, 500]\n",
    "neighbors = np.arange(1, 9)\n",
    "train_accuracy = np.empty(len(neighbors))\n",
    "test_accuracy = np.empty(len(neighbors))\n",
    "for i in range(0, 8):\n",
    "    print(n_range[i])\n",
    "    gbc = GradientBoostingClassifier(n_estimators=(n_range[i]), max_depth=3, learning_rate=0.1, random_state=42)\n",
    "    gbc.fit(X_train, y_train.ravel())\n",
    "    train_accuracy[i] = gbc.score(X_train, y_train)\n",
    "    test_accuracy[i] = gbc.score(X_test, y_test)\n",
    "best_index = np.argmax(test_accuracy)\n",
    "best_alpha = depth_values[best_index]\n",
    "best_test_acc = test_accuracy[best_index]\n",
    "print(f\"GBC Feature selection 1: Highest Test Accuracy: {best_test_acc:.4f} at depth = {best_alpha}\")\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy')\n",
    "plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('[1, 5, 10, 20, 50, 100, 200, 1000] n_estimator values')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# GBC feature selection #2\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X, y)\n",
    "selector = SelectFromModel(rf, threshold=\"median\") #selects above the median\n",
    "selector.fit(X, y)\n",
    "X_selected = selector.transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "gbc = GradientBoostingClassifier(n_estimators=500, max_depth=3, learning_rate=0.1, random_state=42)\n",
    "gbc.fit(X_train, y_train)\n",
    "y_pred = gbc.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy with RF-selected features: {accuracy:.4f}\")\n",
    "selected_features = selector.get_support(indices=True)\n",
    "print(f\"Selected feature indices: {selected_features}\")\n",
    "\n",
    "# GBC Feature importance #1\n",
    "lengthOfFeaturesInOrder = np.arange(1, 257)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "gbc = GradientBoostingClassifier(n_estimators=500, max_depth=3, learning_rate=0.1, random_state=42)\n",
    "gbc.fit(X_train, y_train)\n",
    "importances = gbc.feature_importances_\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.barh(lengthOfFeaturesInOrder, importances, color='skyblue')\n",
    "plt.xlabel('GradientBoostingClassifier Importance')\n",
    "plt.title('Feature Importance - GBC Importance')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis for better visualization\n",
    "plt.show()\n",
    "\n",
    "# GBC Feature importance #2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "gbc = GradientBoostingClassifier(n_estimators=500, max_depth=3, learning_rate=0.1, random_state=42)\n",
    "gbc.fit(X_train, y_train)\n",
    "RF_feature_weights = gbc.feature_importances_\n",
    "feature_weights=RF_feature_weights.flatten()\n",
    "# Create a DataFrame to store the features and their corresponding weights\n",
    "features_df = pd.DataFrame({\n",
    "    'Feature': ds.columns,\n",
    "    'Weight': feature_weights\n",
    "})\n",
    "# Sort by the absolute value of weights to see the most important features\n",
    "features_df['Absolute Weight'] = np.abs(features_df['Weight'])\n",
    "features_df = features_df.sort_values(by='Absolute Weight', ascending=False)\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"Feature\", y=\"Weight\", data=features_df.head(20), palette=\"coolwarm\")\n",
    "plt.title(\"Top 10 Features by Weight in RF Model\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Weight\")\n",
    "# Rotate the x labels by 45 degrees\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plotting trees from GBC\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "gbc = GradientBoostingClassifier(n_estimators=500, max_depth=3, learning_rate=0.1, random_state=42)\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "tree.plot_tree(gbc.estimators_[0,0], filled=True)\n",
    "plt.title(\"First Tree in GBR\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the last tree\n",
    "plt.figure(figsize=(12, 5))\n",
    "tree.plot_tree(gbc.estimators_[-1,0], filled=True)\n",
    "plt.title(\"Last Tree in GBR\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define normalization methods\n",
    "scaling_methods = {\n",
    "    \"No Scaling\": None,\n",
    "    \"MinMax\": MinMaxScaler(),\n",
    "    \"Standard\": StandardScaler(),\n",
    "}\n",
    "\n",
    "# Define scoring metrics including AUC-ROC\n",
    "scoring = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"roc_auc\": make_scorer(roc_auc_score, multi_class=\"ovr\"),  # AUC-ROC for multi-class classification\n",
    "    \"f1\": make_scorer(f1_score, average=\"weighted\"),\n",
    "}\n",
    "\n",
    "# Prepare data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Results container\n",
    "results = []\n",
    "\n",
    "# Iterate over scaling methods\n",
    "for scale_name, scaler in scaling_methods.items():\n",
    "    print(f\"Evaluating Scaling Method: {scale_name}\")\n",
    "\n",
    "    # Build pipeline\n",
    "    steps = []\n",
    "    if scaler is not None:\n",
    "        steps.append((\"scaler\", scaler))\n",
    "    steps.append((\"svc\", SVC(kernel=\"rbf\", random_state=42)))\n",
    "    pipeline = Pipeline(steps)\n",
    "\n",
    "    # GridSearchCV setup\n",
    "    param_grid = {\"svc__C\": [0.1, 1, 10, 100], \"svc__gamma\": [0.001, 0.01, 0.1, 1]}\n",
    "    skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        cv=skf,\n",
    "        scoring=scoring,\n",
    "        refit=\"accuracy\",\n",
    "        n_jobs=-1,\n",
    "        verbose=3,\n",
    "    )\n",
    "\n",
    "    # Fit GridSearchCV\n",
    "    grid_search.fit(X_train, y_train.ravel())\n",
    "\n",
    "    # Extract best scores\n",
    "    best_train_accuracy = grid_search.best_score_\n",
    "    best_model = grid_search.best_estimator_\n",
    "    test_predictions = best_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "\n",
    "    # Store results for each scaling method\n",
    "    results.append(\n",
    "        {\n",
    "            \"Scaling Method\": scale_name,\n",
    "            \"Best Train Accuracy\": best_train_accuracy,\n",
    "            \"Test Accuracy\": test_accuracy,\n",
    "            \"Best Params\": grid_search.best_params_,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Display results for each metric\n",
    "for metric in scoring.keys():\n",
    "    print(f\"\\nBest result for {metric}:\")\n",
    "    best_metric_result = results_df.sort_values(\n",
    "        by=f\"Best Train Accuracy\", ascending=False\n",
    "    ).head(1)\n",
    "    print(best_metric_result)\n",
    "\n",
    "# Display best normalization method overall\n",
    "print(\"\\nBest Normalization Method Overall:\")\n",
    "best_method = results_df.loc[results_df[\"Best Train Accuracy\"].idxmax()]\n",
    "print(best_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to NumPy\n",
    "# X = ds.to_numpy()\n",
    "# y = y.to_numpy()\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),  # Standardize features\n",
    "        (\n",
    "            \"svm\",\n",
    "            SVC(kernel=\"rbf\", probability=True),\n",
    "        ),  # SVM with RBF kernel and probability estimates\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    \"svm__C\": [0.1, 10, 100],  # C values\n",
    "    \"svm__gamma\": [0.01, 0.1, 1],  # Gamma values\n",
    "}\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"roc_auc\": make_scorer(roc_auc_score, multi_class=\"ovr\"),  # AUC-ROC for multi-class classification\n",
    "    \"f1\": make_scorer(f1_score, average=\"weighted\"),\n",
    "}\n",
    "\n",
    "# Setup StratifiedKFold for cross-validation\n",
    "cv = StratifiedKFold(n_splits=4)\n",
    "\n",
    "# Use GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring=scoring,\n",
    "    refit=\"accuracy\",  # Choose the main metric for refitting the best model\n",
    "    cv=cv,\n",
    "    verbose=3,\n",
    "    return_train_score=True,\n",
    ")\n",
    "\n",
    "# Fit the GridSearchCV to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract results into a DataFrame\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Display the top 5 results sorted by validation accuracy\n",
    "top_results = results_df.sort_values(by=\"mean_test_accuracy\", ascending=False).head(5)\n",
    "print(\"Top 5 Hyperparameter Combinations:\")\n",
    "print(\n",
    "    top_results[\n",
    "        [\n",
    "            \"param_svm__C\",\n",
    "            \"param_svm__gamma\",\n",
    "            \"mean_test_accuracy\",\n",
    "            \"mean_test_f1\",\n",
    "            \"mean_test_roc_auc\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Best parameters\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"\\nBest Parameters: {best_params}\")\n",
    "print(f\"Best Validation Accuracy: {best_score:.4f}\")\n",
    "\n",
    "# Evaluate on test data\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_score:.4f}\")\n",
    "\n",
    "# Visualization: Multiple Metrics for Best Hyperparameters\n",
    "metrics = [\n",
    "    \"mean_test_accuracy\",\n",
    "    \"mean_test_f1\",\n",
    "    \"mean_test_roc_auc\",  \n",
    "]\n",
    "best_results = results_df[results_df[\"param_svm__C\"] == best_params[\"svm__C\"]]\n",
    "best_results = best_results[\n",
    "    best_results[\"param_svm__gamma\"] == best_params[\"svm__gamma\"]\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(\n",
    "    metrics,\n",
    "    best_results[metrics].values.flatten(),\n",
    "    color=[\"blue\", \"orange\", \"green\", \"red\", \"purple\"],  # Adding color for AUC-ROC\n",
    ")\n",
    "plt.title(\"Performance Metrics for Best Hyperparameters\", fontsize=14)\n",
    "plt.ylabel(\"Score\", fontsize=12)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),  # Standardize the features\n",
    "        (\n",
    "            \"svm\",\n",
    "            SVC(kernel=\"poly\", probability=True),\n",
    "        ),  # SVM with polynomial kernel and probability estimates\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    \"svm__C\": [0.1, 10, 100],  # Regularization parameter\n",
    "    \"svm__gamma\": [0.01, 0.1, 1],  # Kernel coefficient\n",
    "    \"svm__degree\": [2, 3, 4],  # Degree of the polynomial kernel\n",
    "}\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"roc_auc\": make_scorer(\n",
    "        roc_auc_score, multi_class=\"ovr\"),  # AUC-ROC for multi-class classification\n",
    "    \"f1\": make_scorer(f1_score, average=\"weighted\"),\n",
    "}\n",
    "\n",
    "# Set up Stratified K-Fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=4)\n",
    "\n",
    "# Use GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring=scoring,\n",
    "    refit=\"accuracy\",  # Use accuracy for selecting the best model\n",
    "    cv=cv,\n",
    "    return_train_score=True,\n",
    "    verbose=3,\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract results into a DataFrame\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Display top 5 results sorted by validation accuracy\n",
    "top_results = results_df.sort_values(by=\"mean_test_accuracy\", ascending=False).head(5)\n",
    "print(\"Top 5 Hyperparameter Combinations:\")\n",
    "print(\n",
    "    top_results[\n",
    "        [\n",
    "            \"param_svm__C\",\n",
    "            \"param_svm__gamma\",\n",
    "            \"param_svm__degree\",\n",
    "            \"mean_test_accuracy\",\n",
    "            \"mean_test_f1\",\n",
    "            \"mean_test_roc_auc\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Best parameters and score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"\\nBest Parameters: {best_params}\")\n",
    "print(f\"Best Validation Accuracy: {best_score:.4f}\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_accuracy = grid_search.score(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Visualization of metrics for the best hyperparameters\n",
    "metrics = [\n",
    "    \"mean_test_accuracy\",\n",
    "    \"mean_test_f1\",\n",
    "    \"mean_test_roc_auc\",\n",
    "]\n",
    "best_results = results_df[\n",
    "    (results_df[\"param_svm__C\"] == best_params[\"svm__C\"])\n",
    "    & (results_df[\"param_svm__gamma\"] == best_params[\"svm__gamma\"])\n",
    "    & (results_df[\"param_svm__degree\"] == best_params[\"svm__degree\"])\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(\n",
    "    metrics,\n",
    "    best_results[metrics].values.flatten(),\n",
    "    color=[\"blue\", \"orange\", \"green\", \"red\", \"purple\"],\n",
    ")\n",
    "plt.title(\"Performance Metrics for Best Hyperparameters\", fontsize=14)\n",
    "plt.ylabel(\"Score\", fontsize=12)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Section Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "svm = SVC(kernel='rbf', probability=True, C=100)\n",
    "svm.fit(X_train, y_train.ravel())\n",
    "\n",
    "y_probMLP = svm.predict_proba(X_test)\n",
    "num_classes = y_probMLP.shape[1]\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(num_classes):\n",
    "    plt.subplot(1, num_classes, i + 1)\n",
    "    plt.scatter(y_test, y_probMLP[:, i], alpha=0.5)\n",
    "    plt.title(f'Class {i} Probability')\n",
    "    plt.xlabel('True Labels')\n",
    "    plt.ylabel('Predicted Probability')\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Algorithm with Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define scaling methods\n",
    "scalers = {\"No Scaling\": None, \"MinMax\": MinMaxScaler(), \"Standard\": StandardScaler()}\n",
    "\n",
    "# Define the MLP pipeline\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", None),  # Placeholder for scaler\n",
    "        (\"mlp\", MLPClassifier(max_iter=500, random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    \"scaler\": [None, MinMaxScaler(), StandardScaler()],  # Scaling methods\n",
    "    \"mlp__hidden_layer_sizes\": [(64,), (128,), (64, 64)],  # Hidden layer configurations\n",
    "    \"mlp__activation\": [\"relu\", \"tanh\"],  # Activation functions\n",
    "    \"mlp__alpha\": [0.0001, 0.001],  # Regularization parameter\n",
    "    \"mlp__learning_rate\": [\"constant\", \"adaptive\"],  # Learning rate schedules\n",
    "}\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"f1\": make_scorer(f1_score, average=\"macro\"),\n",
    "    \"aucroc\": make_scorer(roc_auc_score, multi_class=\"ovo\", needs_proba=True),\n",
    "}\n",
    "\n",
    "# Stratified K-Fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=4)\n",
    "\n",
    "# GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring=scoring,\n",
    "    refit=\"accuracy\",  # Use accuracy to select the best model\n",
    "    cv=cv,\n",
    "    verbose=3,\n",
    "    return_train_score=True,\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract results into a DataFrame\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Display the best hyperparameter combination for each scoring metric\n",
    "for metric in scoring.keys():\n",
    "    top_results_metric = results_df.sort_values(\n",
    "        by=f\"mean_test_{metric}\", ascending=False\n",
    "    ).head(1)\n",
    "    print(f\"\\nBest result for {metric}:\")\n",
    "    print(\n",
    "        top_results_metric[\n",
    "            [\n",
    "                \"param_scaler\",\n",
    "                \"param_mlp__hidden_layer_sizes\",\n",
    "                \"param_mlp__activation\",\n",
    "                \"param_mlp__alpha\",\n",
    "                \"param_mlp__learning_rate\",\n",
    "                f\"mean_test_{metric}\",\n",
    "            ]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Best parameters and score (accuracy-based)\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"\\nBest Parameters (Based on Accuracy): {best_params}\")\n",
    "print(f\"Best Validation Accuracy: {best_score:.4f}\")\n",
    "\n",
    "# Test the best model on the test set\n",
    "test_accuracy = grid_search.score(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Plot validation accuracy for each scaling method\n",
    "scaling_methods = [\"No Scaling\", \"MinMaxScaler()\", \"StandardScaler()\"]\n",
    "plt.figure(figsize=(10, 6))\n",
    "for scale_method in scaling_methods:\n",
    "    subset = results_df[results_df[\"param_scaler\"].astype(str) == scale_method]\n",
    "    plt.plot(\n",
    "        subset[\"param_mlp__alpha\"],\n",
    "        subset[\"mean_test_accuracy\"],\n",
    "        label=f\"Validation Accuracy ({scale_method})\",\n",
    "        marker=\"o\",\n",
    "    )\n",
    "\n",
    "# Configure plot\n",
    "plt.xscale(\"log\")  # Log scale for alpha\n",
    "plt.xlabel(\"Alpha (Regularization Parameter)\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.title(\"Validation Accuracy vs. Alpha for Different Scaling Methods\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layer Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define pipeline\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", MinMaxScaler()),  # Scaling using MinMaxScaler\n",
    "        (\"mlp\", MLPClassifier(max_iter=300, random_state=42)),  # MLP Classifier\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define hyperparameter grid for hidden layer sizes\n",
    "layer_configs = [(1,), (2,), (3,)]  # Number of layers\n",
    "node_counts = [64, 128, 256]  # Nodes per layer\n",
    "hidden_layer_combinations = [\n",
    "    tuple([nodes] * layers[0]) for layers in layer_configs for nodes in node_counts\n",
    "]\n",
    "\n",
    "param_grid = {\n",
    "    \"mlp__hidden_layer_sizes\": hidden_layer_combinations,  # Configurations of hidden layers\n",
    "    \"mlp__activation\": [\"relu\", \"tanh\"],  # Activation functions\n",
    "    \"mlp__alpha\": [0.0001, 0.001],  # Regularization strengths\n",
    "}\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"roc_auc\": make_scorer(roc_auc_score, multi_class=\"ovr\"),  # AUC-ROC for multi-class classification\n",
    "    \"f1\": make_scorer(f1_score, average=\"weighted\"),\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "cv = StratifiedKFold(n_splits=4)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring=scoring,\n",
    "    refit=\"accuracy\",  # Use accuracy as the primary metric for model selection\n",
    "    cv=cv,\n",
    "    verbose=3,\n",
    "    return_train_score=True,\n",
    ")\n",
    "\n",
    "# Fit the GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract results into a DataFrame\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Display best results for each scoring type\n",
    "for metric in scoring.keys():\n",
    "    top_results_metric = results_df.sort_values(\n",
    "        by=f\"mean_test_{metric}\", ascending=False\n",
    "    ).head(1)\n",
    "    print(f\"\\nBest result for {metric}:\")\n",
    "    print(\n",
    "        top_results_metric[\n",
    "            [\n",
    "                \"param_mlp__hidden_layer_sizes\",\n",
    "                \"param_mlp__activation\",\n",
    "                \"param_mlp__alpha\",\n",
    "                f\"mean_test_{metric}\",\n",
    "            ]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Best parameters and score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"\\nBest Parameters: {best_params}\")\n",
    "print(f\"Best Validation Accuracy: {best_score:.4f}\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_accuracy = grid_search.score(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot validation accuracy for different configurations\n",
    "for activation in [\"relu\", \"tanh\"]:\n",
    "    subset = results_df[results_df[\"param_mlp__activation\"] == activation]\n",
    "    plt.plot(\n",
    "        subset[\"param_mlp__alpha\"],\n",
    "        subset[\"mean_test_accuracy\"],\n",
    "        label=f\"Activation: {activation}\",\n",
    "        marker=\"o\",\n",
    "    )\n",
    "\n",
    "# Configure plot\n",
    "plt.xscale(\"log\")  # Log scale for alpha\n",
    "plt.xlabel(\"Alpha (Regularization Parameter)\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.title(\"Validation Accuracy vs. Alpha for Different Activation Functions\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),  # Scaling features\n",
    "        (\"mlp\", MLPClassifier(random_state=0, max_iter=300)),  # MLP Classifier\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    \"mlp__solver\": [\"lbfgs\", \"adam\"],  # Solvers\n",
    "    \"mlp__alpha\": [0.1, 1, 10, 100],  # Regularization strengths\n",
    "    \"mlp__hidden_layer_sizes\": [\n",
    "        [10, 10],\n",
    "        [10],\n",
    "        [10, 10, 10],\n",
    "    ],  # Hidden layer configurations\n",
    "}\n",
    "\n",
    "# Define scoring metrics including AUC-ROC\n",
    "scoring = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"roc_auc\": make_scorer(roc_auc_score, multi_class=\"ovr\"),  # AUC-ROC for multi-class classification\n",
    "    \"f1\": make_scorer(f1_score, average=\"weighted\"),\n",
    "}\n",
    "\n",
    "# Set up StratifiedKFold for consistent splits\n",
    "cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=50)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring=scoring,\n",
    "    refit=\"accuracy\",  # Use accuracy as the main metric for selecting the best model\n",
    "    cv=cv,\n",
    "    verbose=2,\n",
    "    return_train_score=True,\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract results into a DataFrame\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Display top configurations based on validation accuracy\n",
    "top_results = results_df.sort_values(by=\"mean_test_accuracy\", ascending=False).head(5)\n",
    "print(\"\\nTop 5 Configurations by Validation Accuracy:\")\n",
    "print(\n",
    "    top_results[\n",
    "        [\n",
    "            \"param_mlp__solver\",\n",
    "            \"param_mlp__alpha\",\n",
    "            \"param_mlp__hidden_layer_sizes\",\n",
    "            \"mean_test_accuracy\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Display best result for each metric\n",
    "for metric in scoring.keys():\n",
    "    top_result_metric = results_df.sort_values(\n",
    "        by=f\"mean_test_{metric}\", ascending=False\n",
    "    ).head(1)\n",
    "    print(f\"\\nBest result for {metric}:\")\n",
    "    print(\n",
    "        top_result_metric[\n",
    "            [\n",
    "                \"param_mlp__solver\",\n",
    "                \"param_mlp__alpha\",\n",
    "                \"param_mlp__hidden_layer_sizes\",\n",
    "                f\"mean_test_{metric}\",\n",
    "            ]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Best parameters and validation accuracy\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"\\nBest Parameters: {best_params}\")\n",
    "print(f\"Best Validation Accuracy: {best_score:.4f}\")\n",
    "\n",
    "# Test set evaluation\n",
    "test_accuracy = grid_search.score(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Visualization of train and validation scores for regularization strength (alpha)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Filter results for specific solver to plot\n",
    "for solver in [\"lbfgs\", \"adam\"]:\n",
    "    subset = results_df[results_df[\"param_mlp__solver\"] == solver]\n",
    "    plt.plot(\n",
    "        subset[\"param_mlp__alpha\"],\n",
    "        subset[\"mean_test_accuracy\"],\n",
    "        label=f\"Solver: {solver}\",\n",
    "        marker=\"o\",\n",
    "    )\n",
    "\n",
    "# Configure plot\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Alpha (Regularization Strength)\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.title(\"Validation Accuracy vs. Alpha for Different Solvers\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Predicting Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP predicting pobability ----------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "mlp = MLPClassifier(solver='lbfgs', random_state=0, alpha=100, hidden_layer_sizes=[10])\n",
    "mlp.fit(X_train, y_train.ravel())\n",
    "\n",
    "y_probMLP = mlp.predict_proba(X_test)\n",
    "num_classes = y_probMLP.shape[1]\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(num_classes):\n",
    "    plt.subplot(1, num_classes, i + 1)\n",
    "    plt.scatter(y_test, y_probMLP[:, i], alpha=0.5)\n",
    "    plt.title(f'Class {i} Probability')\n",
    "    plt.xlabel('True Labels')\n",
    "    plt.ylabel('Predicted Probability')\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM and MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC and MLP together:  --------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "Xtrain = scaler.fit_transform(X_train)\n",
    "Xtest = scaler.transform(X_test)\n",
    "\n",
    "mlp = MLPClassifier(solver='lbfgs', random_state=0, alpha=100, hidden_layer_sizes=[10])\n",
    "mlp.fit(Xtrain, y_train.ravel())\n",
    "\n",
    "svm = SVC(kernel='rbf', probability=True, C=100)\n",
    "svm.fit(Xtrain, y_train.ravel())\n",
    "\n",
    "proba1 = mlp.predict_proba(Xtest)[:, 1]\n",
    "proba2 = svm.predict_proba(Xtest)[:, 1]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_test_numeric = label_encoder.fit_transform(y_test)\n",
    "\n",
    "plt.scatter(proba1, proba2, c=y_test_numeric, cmap='viridis')\n",
    "plt.xlabel(\"Model 1 Probability\")\n",
    "plt.ylabel(\"Model 2 Probability\")\n",
    "plt.title(\"Scatter Plot of Predicted Probabilities\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "svm_probs = svm.predict_proba(Xtest)\n",
    "mlp_probs = mlp.predict_proba(Xtest)\n",
    "\n",
    "# Scatter plots for each class\n",
    "class_labels = {0: 'person', 1: 'sign', 2: 'bike', 3: 'bus', 4: 'car'}\n",
    "n_classes = len(class_labels)\n",
    "fig, axes = plt.subplots(1, n_classes, figsize=(15, 4))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.scatter(svm_probs[:, i], mlp_probs[:, i], alpha=0.5)\n",
    "    ax.set_title(f'Class {class_labels[i]}')\n",
    "    ax.set_xlabel('SVM Probability')\n",
    "    ax.set_ylabel('MLP Probability')\n",
    "\n",
    "plt.suptitle('SVM vs. MLP Probability Scatter Plots per Class')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterRange = range(2, 26)\n",
    "sse = []\n",
    "\n",
    "print(\"Number of Clusters (k) | SSE\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for k in clusterRange:\n",
    "    kMeans = KMeans(\n",
    "        n_clusters=k,\n",
    "        init=\"k-means++\",\n",
    "        n_init=10,\n",
    "        max_iter=500,\n",
    "        tol=1e-04,\n",
    "        random_state=0,\n",
    "    )\n",
    "    kMeans.fit(X)\n",
    "    sseValue = kMeans.inertia_\n",
    "    sse.append(sseValue)\n",
    "    print(f\"{k:<20} | {sseValue:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(clusterRange, sse, marker=\"o\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Sum of Squared Errors (SSE)\")\n",
    "plt.title(\"Elbow Method for Optimal k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = [ ]\n",
    "for i in range(5, 16):\n",
    "    print(i)\n",
    "    km = KMeans(n_clusters=i, init='k-means++',n_init=10, max_iter=300,random_state=0)\n",
    "    km.fit(X)\n",
    "    distortions.append(km.inertia_)\n",
    "plt.plot(range(5,16), distortions, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Distortion')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality to 2D with PCA\n",
    "pca = PCA(n_components=2)\n",
    "data_2d = pca.fit_transform(X)\n",
    "num_clusters_list = [5, 6, 7]\n",
    "# num_clusters_list = [8, 9, 10]\n",
    "# num_clusters_list = [11, 12, 13, 14, 15]\n",
    "# num_clusters_list = [14, 15]\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, len(num_clusters_list), figsize=(16, 4))\n",
    "fig.suptitle(\"K-means Clustering with Different Number of Clusters\")\n",
    "\n",
    "# Plot each clustering result\n",
    "for i, n_clusters in enumerate(num_clusters_list):\n",
    "    # Perform K-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    # Scatter plot of the clusters\n",
    "    ax = axes[i]\n",
    "    for cluster in range(n_clusters):\n",
    "        cluster_points = data_2d[labels == cluster]\n",
    "        ax.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {cluster + 1}')\n",
    "    \n",
    "    # Plot centroids\n",
    "    centroids_2d = pca.transform(kmeans.cluster_centers_)\n",
    "    ax.scatter(centroids_2d[:, 0], centroids_2d[:, 1], s=200, c='black', marker='X', label='Centroids')\n",
    "\n",
    "    ax.set_title(f\"{n_clusters} Clusters\")\n",
    "    ax.set_xlabel(\"Principal Component 1\")\n",
    "    ax.set_ylabel(\"Principal Component 2\")\n",
    "    ax.legend()\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout for the main title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=5, init='k-means++', n_init=10, max_iter=300, tol=1e-04, random_state=0)\n",
    "y_km = km.fit_predict(X)\n",
    "\n",
    "# Calculate silhouette scores\n",
    "cluster_labels = np.unique(y_km)\n",
    "n_clusters = cluster_labels.shape[0]\n",
    "silhouette_vals = silhouette_samples(X, y_km, metric='euclidean')\n",
    "\n",
    "# Plot silhouette scores for each cluster\n",
    "y_ax_lower, y_ax_upper = 0, 0\n",
    "yticks = []\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, c in enumerate(cluster_labels):\n",
    "    # Collect and sort silhouette scores for samples in this cluster\n",
    "    c_silhouette_vals = silhouette_vals[y_km == c]\n",
    "    c_silhouette_vals.sort()\n",
    "    \n",
    "    # Update y-axis limits for each cluster’s silhouette plot\n",
    "    y_ax_upper += len(c_silhouette_vals)\n",
    "    color = cm.jet(float(i) / n_clusters)\n",
    "    plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, edgecolor='none', color=color)\n",
    "    \n",
    "    # Update y-ticks for each cluster\n",
    "    yticks.append((y_ax_lower + y_ax_upper) / 2.)\n",
    "    y_ax_lower += len(c_silhouette_vals)\n",
    "\n",
    "# Plot the average silhouette score as a red dashed line\n",
    "silhouette_avg = np.mean(silhouette_vals)\n",
    "plt.axvline(silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "# Labeling and layout adjustments\n",
    "plt.yticks(yticks, cluster_labels + 1)\n",
    "plt.ylabel('Cluster')\n",
    "plt.xlabel('Silhouette coefficient')\n",
    "plt.title(\"Silhouette Plot for K-means Clustering with 5 Clusters\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(5,16):\n",
    "    km = KMeans(n_clusters=idx, init='k-means++', n_init=10, max_iter=300, tol=1e-04, random_state=0)\n",
    "    y_km = km.fit_predict(X)\n",
    "    ari = adjusted_rand_score(y.ravel(), y_km)\n",
    "    print(f\"Cluster: {idx}. Adjusted Rand Index: {ari}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_array = ward(X)\n",
    "dendrogram(linkage_array)\n",
    "ax = plt.gca()\n",
    "bounds = ax.get_xbound()\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.ylabel(\"Cluster distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit AgglomerativeClustering\n",
    "agg_clust = AgglomerativeClustering(n_clusters=5, linkage='ward')\n",
    "labels = agg_clust.fit_predict(X)  # Use this to get the labels\n",
    "\n",
    "# Calculate silhouette scores\n",
    "silhouette_vals = silhouette_samples(X, labels, metric='euclidean')\n",
    "n_clusters = len(np.unique(labels))\n",
    "\n",
    "# Plot silhouette scores for each cluster\n",
    "y_ax_lower, y_ax_upper = 0, 0\n",
    "yticks = []\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(n_clusters):\n",
    "    # Collect and sort silhouette scores for samples in this cluster\n",
    "    c_silhouette_vals = silhouette_vals[labels == i]\n",
    "    c_silhouette_vals.sort()\n",
    "    \n",
    "    # Update y-axis limits for each cluster’s silhouette plot\n",
    "    y_ax_upper += len(c_silhouette_vals)\n",
    "    color = cm.jet(float(i) / n_clusters)\n",
    "    plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, edgecolor='none', color=color)\n",
    "    \n",
    "    # Update y-ticks for each cluster\n",
    "    yticks.append((y_ax_lower + y_ax_upper) / 2.)\n",
    "    y_ax_lower += len(c_silhouette_vals)\n",
    "\n",
    "# Plot the average silhouette score as a red dashed line\n",
    "silhouette_avg = np.mean(silhouette_vals)\n",
    "plt.axvline(silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "# Labeling and layout adjustments\n",
    "plt.yticks(yticks, range(1, n_clusters + 1))\n",
    "plt.ylabel('Cluster')\n",
    "plt.xlabel('Silhouette coefficient')\n",
    "plt.title(\"Silhouette Plot for Agglomerative Clustering with 5 Clusters\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_array = linkage(X[:100], method='single')\n",
    "dendrogram(linkage_array)\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.ylabel(\"Cluster distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit AgglomerativeClustering\n",
    "agg_clust = AgglomerativeClustering(n_clusters=5, linkage='single')\n",
    "labels = agg_clust.fit_predict(X)  # Use this to get the labels\n",
    "\n",
    "# Calculate silhouette scores\n",
    "silhouette_vals = silhouette_samples(X, labels, metric='euclidean')\n",
    "n_clusters = len(np.unique(labels))\n",
    "\n",
    "# Plot silhouette scores for each cluster\n",
    "y_ax_lower, y_ax_upper = 0, 0\n",
    "yticks = []\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(n_clusters):\n",
    "    # Collect and sort silhouette scores for samples in this cluster\n",
    "    c_silhouette_vals = silhouette_vals[labels == i]\n",
    "    c_silhouette_vals.sort()\n",
    "    \n",
    "    # Update y-axis limits for each cluster’s silhouette plot\n",
    "    y_ax_upper += len(c_silhouette_vals)\n",
    "    color = cm.jet(float(i) / n_clusters)\n",
    "    plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, edgecolor='none', color=color)\n",
    "    \n",
    "    # Update y-ticks for each cluster\n",
    "    yticks.append((y_ax_lower + y_ax_upper) / 2.)\n",
    "    y_ax_lower += len(c_silhouette_vals)\n",
    "\n",
    "# Plot the average silhouette score as a red dashed line\n",
    "silhouette_avg = np.mean(silhouette_vals)\n",
    "plt.axvline(silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "# Labeling and layout adjustments\n",
    "plt.yticks(yticks, range(1, n_clusters + 1))\n",
    "plt.ylabel('Cluster')\n",
    "plt.xlabel('Silhouette coefficient')\n",
    "plt.title(\"Silhouette Plot for Agglomerative Clustering with 5 Clusters\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_array = linkage(X, method='complete')\n",
    "dendrogram(linkage_array)\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.ylabel(\"Cluster distance\")\n",
    "plt.show()\n",
    "\n",
    "#---------------------------- SIL PART THREE\n",
    "\n",
    "# Fit AgglomerativeClustering\n",
    "agg_clust = AgglomerativeClustering(n_clusters=5, linkage='complete')\n",
    "labels = agg_clust.fit_predict(X)  # Use this to get the labels\n",
    "\n",
    "# Calculate silhouette scores\n",
    "silhouette_vals = silhouette_samples(X, labels, metric='euclidean')\n",
    "n_clusters = len(np.unique(labels))\n",
    "\n",
    "# Plot silhouette scores for each cluster\n",
    "y_ax_lower, y_ax_upper = 0, 0\n",
    "yticks = []\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(n_clusters):\n",
    "    # Collect and sort silhouette scores for samples in this cluster\n",
    "    c_silhouette_vals = silhouette_vals[labels == i]\n",
    "    c_silhouette_vals.sort()\n",
    "    \n",
    "    # Update y-axis limits for each cluster’s silhouette plot\n",
    "    y_ax_upper += len(c_silhouette_vals)\n",
    "    color = cm.jet(float(i) / n_clusters)\n",
    "    plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, edgecolor='none', color=color)\n",
    "    \n",
    "    # Update y-ticks for each cluster\n",
    "    yticks.append((y_ax_lower + y_ax_upper) / 2.)\n",
    "    y_ax_lower += len(c_silhouette_vals)\n",
    "\n",
    "# Plot the average silhouette score as a red dashed line\n",
    "silhouette_avg = np.mean(silhouette_vals)\n",
    "plt.axvline(silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "# Labeling and layout adjustments\n",
    "plt.yticks(yticks, range(1, n_clusters + 1))\n",
    "plt.ylabel('Cluster')\n",
    "plt.xlabel('Silhouette coefficient')\n",
    "plt.title(\"Silhouette Plot for Agglomerative Clustering with 5 Clusters\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the base classifier, we use decision tree (unpruned)\n",
    "tree = DecisionTreeClassifier(criterion='entropy', \n",
    "                              max_depth=None,\n",
    "                              random_state=1)\n",
    "\n",
    "bag = BaggingClassifier(estimator=tree,\n",
    "                        n_estimators=2000, \n",
    "                        max_samples=1.0, #draw 100% of the number of samples (with replacement) for each\n",
    "                        max_features=1.0, #use 100% of the number of features (without replacement)\n",
    "                        bootstrap=True, #sampling with replacement after each\n",
    "                        bootstrap_features=False, \n",
    "                        n_jobs=1, \n",
    "                        random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "tree = tree.fit(X_train, y_train)\n",
    "y_train_pred = tree.predict(X_train)\n",
    "y_test_pred = tree.predict(X_test)\n",
    "\n",
    "tree_train = accuracy_score(y_train, y_train_pred)\n",
    "tree_test = accuracy_score(y_test, y_test_pred)\n",
    "print('Decision tree train/test accuracies %.3f/%.3f'\n",
    "      % (tree_train, tree_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bag = bag.fit(X_train, y_train)\n",
    "y_train_pred = bag.predict(X_train)\n",
    "y_test_pred = bag.predict(X_test)\n",
    "\n",
    "bag_train = accuracy_score(y_train, y_train_pred) \n",
    "bag_test = accuracy_score(y_test, y_test_pred) \n",
    "print('Bagging train/test accuracies %.3f/%.3f'\n",
    "      % (bag_train, bag_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train is your dataset (NumPy array) and y_train is the target labels\n",
    "\n",
    "# Step 1: Handle categorical data by encoding with LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply label encoding to each column of X_train\n",
    "X_train_encoded = np.copy(X_train)  # Create a copy to avoid modifying the original X_train\n",
    "\n",
    "for i in range(X_train.shape[1]):  # Iterate through each column\n",
    "    if isinstance(X_train_encoded[0, i], str):  # Check if the column contains strings (categorical data)\n",
    "        X_train_encoded[:, i] = label_encoder.fit_transform(X_train_encoded[:, i])\n",
    "\n",
    "# If y_train contains categorical values, apply LabelEncoder to it too\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# Step 2: Reduce the dimensionality of X_train to 2D using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_encoded)\n",
    "\n",
    "# Step 3: Create the meshgrid for visualization based on the reduced data\n",
    "x_min = X_train_pca[:, 0].min() - 1\n",
    "x_max = X_train_pca[:, 0].max() + 1\n",
    "y_min = X_train_pca[:, 1].min() - 1\n",
    "y_max = X_train_pca[:, 1].max() + 1\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "# Step 4: Create subplots for both classifiers (Decision Tree and Bagging)\n",
    "f, axarr = plt.subplots(nrows=1, ncols=2, \n",
    "                        sharex='col', \n",
    "                        sharey='row', \n",
    "                        figsize=(8, 3))\n",
    "\n",
    "# Step 5: Train the classifiers and plot decision boundaries\n",
    "for idx, clf, tt in zip([0, 1], [DecisionTreeClassifier(), BaggingClassifier()], ['Decision tree', 'Bagging']):\n",
    "    clf.fit(X_train_pca, y_train_encoded)  # Fit on the 2D PCA data\n",
    "\n",
    "    # Predict on the meshgrid and reshape to match the grid\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot the contour for decision boundary\n",
    "    axarr[idx].contourf(xx, yy, Z, alpha=0.3)\n",
    "\n",
    "    # Scatter the data points\n",
    "    axarr[idx].scatter(X_train_pca[y_train_encoded == 0, 0],\n",
    "                       X_train_pca[y_train_encoded == 0, 1],\n",
    "                       c='blue', marker='^', label='Class 0')\n",
    "    axarr[idx].scatter(X_train_pca[y_train_encoded == 1, 0],\n",
    "                       X_train_pca[y_train_encoded == 1, 1],\n",
    "                       c='green', marker='o', label='Class 1')\n",
    "\n",
    "    axarr[idx].set_title(tt)\n",
    "    axarr[idx].legend()\n",
    "\n",
    "# Add labels and adjust layout\n",
    "axarr[0].set_ylabel('PC 2', fontsize=12)  # Principal Component 2\n",
    "plt.tight_layout()\n",
    "plt.text(0, -0.2,\n",
    "         s='2D Projection of 256 Features',\n",
    "         ha='center',\n",
    "         va='center',\n",
    "         fontsize=12,\n",
    "         transform=axarr[1].transAxes)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADA Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(criterion='entropy', \n",
    "                              max_depth=10,\n",
    "                              random_state=1)\n",
    "\n",
    "ada = AdaBoostClassifier(estimator=tree,\n",
    "                         n_estimators=100, \n",
    "                         learning_rate=0.1, #shrinks the contribution of each classifier\n",
    "                         random_state=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "tree = tree.fit(X_train, y_train)\n",
    "y_train_pred = tree.predict(X_train)\n",
    "y_test_pred = tree.predict(X_test)\n",
    "\n",
    "tree_train = accuracy_score(y_train, y_train_pred)\n",
    "tree_test = accuracy_score(y_test, y_test_pred)\n",
    "print('Decision tree train/test accuracies %.3f/%.3f'\n",
    "      % (tree_train, tree_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = ada.fit(X_train, y_train)\n",
    "y_train_pred = ada.predict(X_train)\n",
    "y_test_pred = ada.predict(X_test)\n",
    "\n",
    "ada_train = accuracy_score(y_train, y_train_pred) \n",
    "ada_test = accuracy_score(y_test, y_test_pred) \n",
    "print('AdaBoost train/test accuracies %.3f/%.3f'\n",
    "      % (ada_train, ada_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume X (256 features) and y (target) are already loaded\n",
    "# For example:\n",
    "# X = np.random.rand(100, 256)  # Replace with your actual data\n",
    "# y = np.random.randint(0, 2, 100)  # Replace with your actual target\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply label encoding to each column of X_train\n",
    "X_train_encoded = np.copy(X_train)  # Create a copy to avoid modifying the original X_train\n",
    "\n",
    "for i in range(X_train.shape[1]):  # Iterate through each column\n",
    "    if isinstance(X_train_encoded[0, i], str):  # Check if the column contains strings (categorical data)\n",
    "        X_train_encoded[:, i] = label_encoder.fit_transform(X_train_encoded[:, i])\n",
    "\n",
    "# If y_train contains categorical values, apply LabelEncoder to it too\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_encoded, y_train_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 1: Apply PCA to reduce the data to 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Step 2: Define classifiers (Decision Tree and AdaBoost)\n",
    "tree = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=1)\n",
    "ada = AdaBoostClassifier(estimator=tree, n_estimators=100, learning_rate=0.5, random_state=1)\n",
    "\n",
    "# Step 3: Create meshgrid for visualization (after PCA)\n",
    "x_min, x_max = X_train_pca[:, 0].min() - 1, X_train_pca[:, 0].max() + 1\n",
    "y_min, y_max = X_train_pca[:, 1].min() - 1, X_train_pca[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "# Step 4: Create subplots for both classifiers\n",
    "f, axarr = plt.subplots(1, 2, sharex='col', sharey='row', figsize=(8, 3))\n",
    "\n",
    "# Step 5: Train both classifiers and visualize decision boundaries\n",
    "for idx, clf, tt in zip([0, 1], [tree, ada], ['Decision tree', 'AdaBoost']):\n",
    "    clf.fit(X_train_pca, y_train)  # Fit the classifier with PCA-reduced data\n",
    "\n",
    "    # Predict on the meshgrid to visualize decision boundaries\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot decision boundary\n",
    "    axarr[idx].contourf(xx, yy, Z, alpha=0.3)\n",
    "\n",
    "    # Plot the training data points\n",
    "    axarr[idx].scatter(X_train_pca[y_train == 0, 0], X_train_pca[y_train == 0, 1], c='blue', marker='^', label='Class 0')\n",
    "    axarr[idx].scatter(X_train_pca[y_train == 1, 0], X_train_pca[y_train == 1, 1], c='green', marker='o', label='Class 1')\n",
    "\n",
    "    # Set title for each subplot\n",
    "    axarr[idx].set_title(tt)\n",
    "    axarr[idx].legend()\n",
    "\n",
    "# Add labels and improve layout\n",
    "axarr[0].set_ylabel(\"Principal Component 2\", fontsize=12)\n",
    "axarr[0].set_xlabel(\n",
    "    \"Principal Component 1\", fontsize=12\n",
    ")  # Add xlabel to the first plot as well\n",
    "axarr[1].set_xlabel(\"Principal Component 1\", fontsize=12)\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout(pad=3.0)  # Increase padding between subplots\n",
    "\n",
    "# Add text below the plot with proper positioning\n",
    "f.text(0.5, 0.02, \"2D Projection of 256 Features with PCA\", ha=\"center\", fontsize=14)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Test Dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finalproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
