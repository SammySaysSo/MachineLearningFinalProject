{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CSE 546: Final Project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries Importation and Data Reading\n",
    "- Import the necessary libraries.\n",
    "- Load and inspect the movie reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import cm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder # found online ~ due to y values not being integers but strings\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"Data.csv\")\n",
    "df2 = pd.read_csv(\"extra_hard_samples.csv\")\n",
    "ds = pd.concat([df1, df2], axis=0).reset_index(drop=True)\n",
    "\n",
    "y = ds[\"class\"]\n",
    "images = ds[\"image_name\"]\n",
    "ds = ds.drop(\"class\", axis=1)\n",
    "ds = ds.drop(\"image_name\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier #1: KNN --------------------------------------------------------------------------\n",
    "# KNN 4-fold cross validation on K parameter: 5 wins - 0.9868 with K = 5.0000e+00\n",
    "X=ds.to_numpy()\n",
    "y=y.to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "k_range = range(1, 25)  # varying k values\n",
    "cv_scores = []\n",
    "average_scores = []\n",
    "train_scores = []\n",
    "val_scores = []\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "for k in k_range:\n",
    "    fold_train_scores = []\n",
    "    fold_val_scores = []\n",
    "    print(\"k running now:\", k)\n",
    "    count = 0\n",
    "    for train_index, test_index in kf.split(X_train, y_train):\n",
    "        count += 1\n",
    "        print(\"Fold running now:\", count)\n",
    "        X_train_fold, X_val_fold = X_train[train_index], X_train[test_index]\n",
    "        y_train_fold, y_val_fold = y_train[train_index], y_train[test_index]\n",
    "\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        fold_train_scores.append(knn.score(X_train_fold, y_train_fold))\n",
    "        fold_val_scores.append(knn.score(X_val_fold, y_val_fold))\n",
    "\n",
    "    train_scores.append(np.mean(fold_train_scores))\n",
    "    val_scores.append(np.mean(fold_val_scores))\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.plot(k_range, train_scores, marker='o', linestyle='-', color='blue', label='Train Accuracy')\n",
    "plt.plot(k_range, val_scores, marker='o', linestyle='-', color='red', label='Validation Accuracy')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Validation Accuracy vs. k')\n",
    "plt.xticks(k_range)\n",
    "plt.ylim(0.1, 1.1)  # set y-axis limits\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "best_index = np.argmax(val_scores)\n",
    "best_alpha = k_range[best_index]\n",
    "best_val_acc = val_scores[best_index]\n",
    "\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.4f} for K = {best_alpha:.4e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Different Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# testing different metrics------------Manhattan wins(k value = 5, train was 0.9890 and validation was 0.9875)\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "X = pd.get_dummies(ds).to_numpy()\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "best_k = 5\n",
    "distance_metrics = ['euclidean', 'manhattan', 'minkowski']\n",
    "train_accuracies = {metric: [] for metric in distance_metrics}\n",
    "val_accuracies = {metric: [] for metric in distance_metrics}\n",
    "\n",
    "for metric in distance_metrics:\n",
    "    train_scores = []\n",
    "    val_scores = []\n",
    "    print(metric)\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        knn = KNeighborsClassifier(n_neighbors=best_k, metric=metric)\n",
    "        knn.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        train_scores.append(knn.score(X_train_fold, y_train_fold))\n",
    "        val_scores.append(knn.score(X_val_fold, y_val_fold))\n",
    "\n",
    "    train_accuracies[metric] = np.mean(train_scores)\n",
    "    val_accuracies[metric] = np.mean(val_scores)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for metric in distance_metrics:\n",
    "    plt.plot([best_k], [train_accuracies[metric]], marker='o', label=f'{metric.capitalize()} (Training Accuracy)', linestyle='--')\n",
    "\n",
    "for metric in distance_metrics:\n",
    "    plt.plot([best_k], [val_accuracies[metric]], marker='s', label=f'{metric.capitalize()} (Validation Accuracy)')\n",
    "\n",
    "plt.title(f\"KNN Accuracy Comparison with Different Distance Metrics (k={best_k})\")\n",
    "plt.xlabel(\"Number of Neighbors (k)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "for metric in distance_metrics:\n",
    "    print(f\"Best k = {best_k}, Distance Metric = {metric.capitalize()}:\")\n",
    "    print(f\"Training Accuracy: {train_accuracies[metric]:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracies[metric]:.4f}\")\n",
    "    print('---')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization - MinMax & Mean-Sigma & Median-MAD & RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN Data normalization #1: MinMax\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(10,10))\n",
    "k_list=np.arange(2,34,1)\n",
    "knn_dict={}\n",
    "for i in k_list:\n",
    "    print(i)\n",
    "    knn=KNeighborsClassifier(n_neighbors=int(i), metric='minkowski')\n",
    "    model_knn=knn.fit(X_train,y_train.ravel())\n",
    "    y_knn_pred=model_knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_knn_pred)\n",
    "    knn_dict[i] = accuracy\n",
    "best_k = max(knn_dict, key=knn_dict.get)\n",
    "best_accuracy = knn_dict[best_k]\n",
    "print(f\"MinMax: Highest Accuracy: {best_accuracy:.4f} at k={best_k}\")\n",
    "ax.plot(knn_dict.keys(),knn_dict.values(), marker='o', linestyle='-')\n",
    "ax.set_xlabel('K-VALUE', fontsize=20)\n",
    "ax.set_ylabel('Accuracy',fontsize=20)\n",
    "ax.set_title('K-Value vs Accuracy', fontsize=22)\n",
    "plt.show()\n",
    "\n",
    "#KNN Data normalization #2: Mean-Sigma\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "if (X < 0).any():\n",
    "    X -= X.min(axis=0)  #make all features non-negative\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(10,10))\n",
    "k_list=np.arange(2,34,1)\n",
    "knn_dict={}\n",
    "for i in k_list:\n",
    "    print(i)\n",
    "    knn=KNeighborsClassifier(n_neighbors=int(i), metric='minkowski')\n",
    "    model_knn=knn.fit(X_train,y_train.ravel())\n",
    "    y_knn_pred=model_knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_knn_pred)\n",
    "    knn_dict[i] = accuracy\n",
    "best_k = max(knn_dict, key=knn_dict.get)\n",
    "best_accuracy = knn_dict[best_k]\n",
    "print(f\"Mean-Sigma: Highest Accuracy: {best_accuracy:.4f} at k={best_k}\")\n",
    "ax.plot(knn_dict.keys(),knn_dict.values(), marker='o', linestyle='-')\n",
    "ax.set_xlabel('K-VALUE', fontsize=20)\n",
    "ax.set_ylabel('Accuracy',fontsize=20)\n",
    "ax.set_title('K-Value vs Accuracy', fontsize=22)\n",
    "plt.show()\n",
    "\n",
    "#KNN Data normalization #3: Median-MAD\n",
    "def median_mad_normalize(X): #SOURCE: google generative ai when searched in google \"median mad normalization in python\"\n",
    "    medians = np.median(X, axis=0)\n",
    "    mad = np.median(np.abs(X - medians), axis=0)\n",
    "    mad[mad == 0] = 1e-6\n",
    "    X_normalized = (X - medians) / mad\n",
    "    return X_normalized\n",
    "X = median_mad_normalize(X)\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(10,10))\n",
    "k_list=np.arange(2,34,1)\n",
    "knn_dict={}\n",
    "for i in k_list:\n",
    "    print(i)\n",
    "    knn=KNeighborsClassifier(n_neighbors=int(i), metric='minkowski')\n",
    "    model_knn=knn.fit(X_train,y_train.ravel())\n",
    "    y_knn_pred=model_knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_knn_pred)\n",
    "    knn_dict[i] = accuracy\n",
    "best_k = max(knn_dict, key=knn_dict.get)\n",
    "best_accuracy = knn_dict[best_k]\n",
    "print(f\"Median-MAD: Highest Accuracy: {best_accuracy:.4f} at k={best_k}\")\n",
    "ax.plot(knn_dict.keys(),knn_dict.values(), marker='o', linestyle='-')\n",
    "ax.set_xlabel('K-VALUE', fontsize=20)\n",
    "ax.set_ylabel('Accuracy',fontsize=20)\n",
    "ax.set_title('K-Value vs Accuracy', fontsize=22)\n",
    "plt.show()\n",
    "\n",
    "#KNN Data normalization #4: robustScaler\n",
    "robust_scaler = RobustScaler()\n",
    "X_selected = robust_scaler.fit_transform(X)\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(10,10))\n",
    "k_list=np.arange(2,34,1)\n",
    "knn_dict={}\n",
    "for i in k_list:\n",
    "    print(i)\n",
    "    knn=KNeighborsClassifier(n_neighbors=int(i), metric='minkowski')\n",
    "    model_knn=knn.fit(X_train,y_train.ravel())\n",
    "    y_knn_pred=model_knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_knn_pred)\n",
    "    knn_dict[i] = accuracy\n",
    "best_k = max(knn_dict, key=knn_dict.get)\n",
    "best_accuracy = knn_dict[best_k]\n",
    "print(f\"RobustScaler: Highest Accuracy: {best_accuracy:.4f} at k={best_k}\")\n",
    "ax.plot(knn_dict.keys(),knn_dict.values(), marker='o', linestyle='-')\n",
    "ax.set_xlabel('K-VALUE', fontsize=20)\n",
    "ax.set_ylabel('Accuracy',fontsize=20)\n",
    "ax.set_title('K-Value vs Accuracy', fontsize=22)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Feature selection #1\n",
    "k = 5  # Number of top features to select ~ modify this value to get different results\n",
    "selector = SelectKBest(score_func=f_classif, k=k)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "fig,ax=plt.subplots(figsize=(10,10))\n",
    "k_list=np.arange(2,34,1)\n",
    "knn_dict={}\n",
    "for i in k_list:\n",
    "    print(i)\n",
    "    knn=KNeighborsClassifier(n_neighbors=int(i), metric='minkowski')\n",
    "    model_knn=knn.fit(X_train,y_train.ravel())\n",
    "    y_knn_pred=model_knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_knn_pred)\n",
    "    knn_dict[i] = accuracy\n",
    "best_k = max(knn_dict, key=knn_dict.get)\n",
    "best_accuracy = knn_dict[best_k]\n",
    "print(f\"Feature Select #1: Highest Accuracy: {best_accuracy:.4f} at k={best_k}\")\n",
    "ax.plot(knn_dict.keys(),knn_dict.values(), marker='o', linestyle='-')\n",
    "ax.set_xlabel('K-VALUE', fontsize=20)\n",
    "ax.set_ylabel('Accuracy',fontsize=20)\n",
    "ax.set_title('K-Value vs Accuracy', fontsize=22)\n",
    "plt.show()\n",
    "\n",
    "# KNN Feature selection #2 - another method found online that uses RFC ------------\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X, y)\n",
    "selector = SelectFromModel(rf, threshold=\"median\") #selects above the median\n",
    "selector.fit(X, y)\n",
    "X_selected = selector.transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Feature Selection #2: Accuracy with RF-selected features: {accuracy:.4f}\")\n",
    "selected_features = selector.get_support(indices=True)\n",
    "print(f\"Feature Selection #2: Selected feature indices: {selected_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNB Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier 2: MNB: 4 fold on MNB: alpha that won = 10^-4, 0.9828\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X) #tells features to be [0,1] to avoid negative bc mnb doesn't work with negative values in data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "alpha_range = np.logspace(-4, 4, 9)  # This creates a range from 1e-4 to 1e4\n",
    "train_scores = []\n",
    "val_scores = []\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=50)\n",
    "\n",
    "for A in alpha_range:\n",
    "    fold_train_scores = []\n",
    "    fold_val_scores = []\n",
    "    print(\"A running now:\", A)\n",
    "    count = 0\n",
    "    for train_index, val_index in kf.split(X_train, y_train):\n",
    "        count += 1\n",
    "        print(\"Fold running now:\", count)\n",
    "        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        mnb = MultinomialNB(alpha=A)\n",
    "        mnb.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        fold_train_scores.append(mnb.score(X_train_fold, y_train_fold))\n",
    "        fold_val_scores.append(mnb.score(X_val_fold, y_val_fold))\n",
    "\n",
    "    train_scores.append(np.mean(fold_train_scores))\n",
    "    val_scores.append(np.mean(fold_val_scores))\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.plot(alpha_range, train_scores, marker='o', linestyle='-', color='blue', label='Train Accuracy')\n",
    "plt.plot(alpha_range, val_scores, marker='o', linestyle='-', color='red', label='Validation Accuracy')\n",
    "plt.xscale('log')  # Use a logarithmic scale for C\n",
    "plt.xlabel('Regularization Strength (C)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Validation Accuracy vs. C')\n",
    "plt.ylim(0.1, 1.1)  # set y-axis limits\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "best_index = np.argmax(val_scores)\n",
    "best_alpha = alpha_range[best_index]\n",
    "best_val_acc = val_scores[best_index]\n",
    "\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.4f} for Alpha = {best_alpha:.4e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNB Data normalization #1: MinMax\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "neighbors = np.arange(1, 20) \n",
    "flag = .00001\n",
    "train_accuracy = np.empty(len(neighbors)) \n",
    "test_accuracy = np.empty(len(neighbors)) \n",
    "alpha_values = []\n",
    "for i in range(0, 19): \n",
    "    flag *= 10\n",
    "    mnb = MultinomialNB(alpha=flag, fit_prior=True)\n",
    "    mnb.fit(X_train, y_train)\n",
    "    train_accuracy[i] = mnb.score(X_train, y_train) \n",
    "    test_accuracy[i] = mnb.score(X_test, y_test) \n",
    "    alpha_values.append(flag)\n",
    "best_index = np.argmax(test_accuracy)\n",
    "best_alpha = alpha_values[best_index]\n",
    "best_test_acc = test_accuracy[best_index]\n",
    "print(f\"MinMax: Highest Test Accuracy: {best_test_acc:.4f} at Alpha = {best_alpha:.4e}\")\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy') \n",
    "plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy') \n",
    "plt.legend() \n",
    "plt.xlabel('run time starting with .0001 then times 10 each run') \n",
    "plt.ylabel('Accuracy') \n",
    "plt.show()\n",
    "\n",
    "# MNB Data normalization #2: Mean-Sigma\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "if (X < 0).any():\n",
    "    X -= X.min(axis=0)  #make all features non-negative\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "neighbors = np.arange(1, 20) \n",
    "flag = .00001\n",
    "train_accuracy = np.empty(len(neighbors)) \n",
    "test_accuracy = np.empty(len(neighbors)) \n",
    "alpha_values = []\n",
    "for i in range(0, 19): \n",
    "    flag *= 10\n",
    "    mnb = MultinomialNB(alpha=flag, fit_prior=True)\n",
    "    mnb.fit(X_train, y_train)\n",
    "    train_accuracy[i] = mnb.score(X_train, y_train) \n",
    "    test_accuracy[i] = mnb.score(X_test, y_test) \n",
    "    alpha_values.append(flag)\n",
    "best_index = np.argmax(test_accuracy)\n",
    "best_alpha = alpha_values[best_index]\n",
    "best_test_acc = test_accuracy[best_index]\n",
    "print(f\"Mean-Sigma: Highest Test Accuracy: {best_test_acc:.4f} at Alpha = {best_alpha:.4e}\")\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy') \n",
    "plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy') \n",
    "plt.legend() \n",
    "plt.xlabel('run time starting with .0001 then times 10 each run') \n",
    "plt.ylabel('Accuracy') \n",
    "plt.show()\n",
    "\n",
    "# MNB Data normalization #3: Median-MAD\n",
    "def median_mad_normalize_nonnegative(data):\n",
    "    medians = np.median(data, axis=0)\n",
    "    mad = np.median(np.abs(data - medians), axis=0)\n",
    "    mad[mad == 0] = 1  # Prevent division by zero\n",
    "    normalized_data = (data - medians) / mad\n",
    "\n",
    "    # Shift to make all values non-negative\n",
    "    min_val = np.min(normalized_data)\n",
    "    if min_val < 0:\n",
    "        normalized_data += abs(min_val) + 1e-6  # Shift to ensure non-negativity\n",
    "    return normalized_data\n",
    "X = median_mad_normalize_nonnegative(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "neighbors = np.arange(1, 20) \n",
    "flag = .00001\n",
    "train_accuracy = np.empty(len(neighbors)) \n",
    "test_accuracy = np.empty(len(neighbors)) \n",
    "alpha_values = []\n",
    "for i in range(0, 19): \n",
    "    flag *= 10\n",
    "    mnb = MultinomialNB(alpha=flag, fit_prior=True)\n",
    "    mnb.fit(X_train, y_train)\n",
    "    train_accuracy[i] = mnb.score(X_train, y_train) \n",
    "    test_accuracy[i] = mnb.score(X_test, y_test) \n",
    "    alpha_values.append(flag)\n",
    "best_index = np.argmax(test_accuracy)\n",
    "best_alpha = alpha_values[best_index]\n",
    "best_test_acc = test_accuracy[best_index]\n",
    "print(f\"Median-MAD: Highest Test Accuracy: {best_test_acc:.4f} at Alpha = {best_alpha:.4e}\")\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy') \n",
    "plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy') \n",
    "plt.legend() \n",
    "plt.xlabel('run time starting with .0001 then times 10 each run') \n",
    "plt.ylabel('Accuracy') \n",
    "plt.show()\n",
    "\n",
    "# MNB Data normalization #4: robustScaler\n",
    "robust_scaler = RobustScaler()\n",
    "X = robust_scaler.fit_transform(X)\n",
    "X = X - X.min(axis=0) #shift so no negatives\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "neighbors = np.arange(1, 20) \n",
    "flag = .00001\n",
    "train_accuracy = np.empty(len(neighbors)) \n",
    "test_accuracy = np.empty(len(neighbors)) \n",
    "alpha_values = []\n",
    "for i in range(0, 19): \n",
    "    flag *= 10\n",
    "    mnb = MultinomialNB(alpha=flag, fit_prior=True)\n",
    "    mnb.fit(X_train, y_train)\n",
    "    train_accuracy[i] = mnb.score(X_train, y_train) \n",
    "    test_accuracy[i] = mnb.score(X_test, y_test) \n",
    "    alpha_values.append(flag)\n",
    "best_index = np.argmax(test_accuracy)\n",
    "best_alpha = alpha_values[best_index]\n",
    "best_test_acc = test_accuracy[best_index]\n",
    "print(f\"Median-MAD: Highest Test Accuracy: {best_test_acc:.4f} at Alpha = {best_alpha:.4e}\")\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy') \n",
    "plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy') \n",
    "plt.legend() \n",
    "plt.xlabel('run time starting with .0001 then times 10 each run') \n",
    "plt.ylabel('Accuracy') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNB Feature selection #1\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "k = 50  # Number of top features to select\n",
    "selector = SelectKBest(score_func=f_classif, k=k)\n",
    "X = selector.fit_transform(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "neighbors = np.arange(1, 20)\n",
    "flag = .00001\n",
    "train_accuracy = np.empty(len(neighbors))\n",
    "test_accuracy = np.empty(len(neighbors))\n",
    "alpha_values = []\n",
    "for i in range(0, 19):\n",
    "    flag *= 10\n",
    "    mnb = MultinomialNB(alpha=flag, fit_prior=True)\n",
    "    mnb.fit(X_train, y_train)\n",
    "    train_accuracy[i] = mnb.score(X_train, y_train)\n",
    "    test_accuracy[i] = mnb.score(X_test, y_test)\n",
    "    alpha_values.append(flag)\n",
    "best_index = np.argmax(test_accuracy)\n",
    "best_alpha = alpha_values[best_index]\n",
    "best_test_acc = test_accuracy[best_index]\n",
    "print(f\"MNB Feature selection 1: Highest Test Accuracy: {best_test_acc:.4f} at Alpha = {best_alpha:.4e}\")\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy')\n",
    "plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('run time starting with .0001 then times 10 each run')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# MNB Feature selection #2 - another method found online that uses RFC\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X, y)\n",
    "selector = SelectFromModel(rf, threshold=\"median\") #selects above the median\n",
    "selector.fit(X, y)\n",
    "X_selected = selector.transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "mnb = MultinomialNB(alpha=100, fit_prior=True) #select optimal value\n",
    "mnb.fit(X_train, y_train)\n",
    "y_pred = mnb.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy with RF-selected features: {accuracy:.4f}\")\n",
    "selected_features = selector.get_support(indices=True)\n",
    "print(f\"Selected feature indices: {selected_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier #3: RF --------------------------------------------------------------------------\n",
    "#4fold on max_depth on RF with n_estimators=1000 => winner: max_depth=None\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "max_depth_values = [None,5, 10, 20, 30, 50, 100]\n",
    "train_scores = []\n",
    "val_scores = []\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=50)\n",
    "\n",
    "for N in max_depth_values:\n",
    "    fold_train_scores = []\n",
    "    fold_val_scores = []\n",
    "    print(\"depth running now:\", N)\n",
    "    count = 0\n",
    "    for train_index, val_index in kf.split(X_train, y_train):\n",
    "        count += 1\n",
    "        print(\"Fold running now:\", count)\n",
    "        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        forest = RandomForestClassifier(n_estimators=1000,max_depth=N, random_state=42)\n",
    "        forest.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        fold_train_scores.append(forest.score(X_train_fold, y_train_fold))\n",
    "        fold_val_scores.append(forest.score(X_val_fold, y_val_fold))\n",
    "\n",
    "    train_scores.append(np.mean(fold_train_scores))\n",
    "    val_scores.append(np.mean(fold_val_scores))\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.plot(max_depth_values, train_scores, marker='o', linestyle='-', color='blue', label='Train Accuracy')\n",
    "plt.plot(max_depth_values, val_scores, marker='o', linestyle='-', color='red', label='Validation Accuracy')\n",
    "plt.xscale('log')  # Use a logarithmic scale for C\n",
    "plt.xlabel('Regularization Strength (C)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Validation Accuracy vs. C')\n",
    "plt.ylim(0.1, 1.1)  # set y-axis limits\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "best_index = np.argmax(val_scores)\n",
    "best_n = max_depth_values[best_index]\n",
    "best_val_acc = val_scores[best_index]\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.4f} for depth = {best_n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF: no preprocessing ----------------------\n",
    "# RF Data normalization #1: MinMax\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "max_depth_values = [None,5, 10, 20, 30, 50, 100]\n",
    "neighbors = np.arange(1, 8) \n",
    "train_accuracy = np.empty(len(neighbors)) \n",
    "test_accuracy = np.empty(len(neighbors)) \n",
    "depth_values = []\n",
    "for i in range(0, 7):\n",
    "    print(max_depth_values[i])\n",
    "    forest = RandomForestClassifier(n_estimators=1000,max_depth=max_depth_values[i], random_state=42)\n",
    "    forest.fit(X_train, y_train)\n",
    "    train_accuracy[i] = forest.score(X_train, y_train) \n",
    "    test_accuracy[i] = forest.score(X_test, y_test) \n",
    "    depth_values.append(max_depth_values[i])\n",
    "best_index = np.argmax(test_accuracy)\n",
    "best_alpha = depth_values[best_index]\n",
    "best_test_acc = test_accuracy[best_index]\n",
    "print(f\"MinMax: Highest Test Accuracy: {best_test_acc:.4f} at depth = {best_alpha}\")\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy') \n",
    "plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy') \n",
    "plt.legend() \n",
    "plt.xlabel('[None,5, 10, 20, 30, 50, 100] max_depth values') \n",
    "plt.ylabel('Accuracy') \n",
    "plt.show()\n",
    "\n",
    "# KNN Data normalization #2: Mean-Sigma\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "if (X < 0).any():\n",
    "    X -= X.min(axis=0)  #make all features non-negative\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "max_depth_values = [None,5, 10, 20, 30, 50, 100]\n",
    "neighbors = np.arange(1, 8) \n",
    "train_accuracy = np.empty(len(neighbors)) \n",
    "test_accuracy = np.empty(len(neighbors)) \n",
    "depth_values = []\n",
    "for i in range(0, 7):\n",
    "    print(max_depth_values[i])\n",
    "    forest = RandomForestClassifier(n_estimators=1000,max_depth=max_depth_values[i], random_state=42)\n",
    "    forest.fit(X_train, y_train)\n",
    "    train_accuracy[i] = forest.score(X_train, y_train) \n",
    "    test_accuracy[i] = forest.score(X_test, y_test) \n",
    "    depth_values.append(max_depth_values[i])\n",
    "best_index = np.argmax(test_accuracy)\n",
    "best_alpha = depth_values[best_index]\n",
    "best_test_acc = test_accuracy[best_index]\n",
    "print(f\"Mean-Sigma: Highest Test Accuracy: {best_test_acc:.4f} at depth = {best_alpha}\")\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy') \n",
    "plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy') \n",
    "plt.legend() \n",
    "plt.xlabel('[None,5, 10, 20, 30, 50, 100] max_depth values') \n",
    "plt.ylabel('Accuracy') \n",
    "plt.show()\n",
    "\n",
    "# KNN Data normalization #3: Median-MAD\n",
    "def median_mad_normalize(X): #SOURCE: google generative ai when searched in google \"median mad normalization in python\"\n",
    "    medians = np.median(X, axis=0)\n",
    "    mad = np.median(np.abs(X - medians), axis=0)\n",
    "    mad[mad == 0] = 1e-6\n",
    "    X_normalized = (X - medians) / mad\n",
    "    return X_normalized\n",
    "X = median_mad_normalize(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "max_depth_values = [None,5, 10, 20, 30, 50, 100]\n",
    "neighbors = np.arange(1, 8) \n",
    "train_accuracy = np.empty(len(neighbors)) \n",
    "test_accuracy = np.empty(len(neighbors)) \n",
    "depth_values = []\n",
    "for i in range(0, 7):\n",
    "    print(max_depth_values[i])\n",
    "    forest = RandomForestClassifier(n_estimators=1000,max_depth=max_depth_values[i], random_state=42)\n",
    "    forest.fit(X_train, y_train)\n",
    "    train_accuracy[i] = forest.score(X_train, y_train) \n",
    "    test_accuracy[i] = forest.score(X_test, y_test) \n",
    "    depth_values.append(max_depth_values[i])\n",
    "best_index = np.argmax(test_accuracy)\n",
    "best_alpha = depth_values[best_index]\n",
    "best_test_acc = test_accuracy[best_index]\n",
    "print(f\"Median-MAD: Highest Test Accuracy: {best_test_acc:.4f} at depth = {best_alpha}\")\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy') \n",
    "plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy') \n",
    "plt.legend() \n",
    "plt.xlabel('[None,5, 10, 20, 30, 50, 100] max_depth values') \n",
    "plt.ylabel('Accuracy') \n",
    "plt.show()\n",
    "\n",
    "# RF Data normalization #4: robustScaler\n",
    "robust_scaler = RobustScaler()\n",
    "X = robust_scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "max_depth_values = [None,5, 10, 20, 30, 50, 100]\n",
    "neighbors = np.arange(1, 8) \n",
    "train_accuracy = np.empty(len(neighbors)) \n",
    "test_accuracy = np.empty(len(neighbors)) \n",
    "depth_values = []\n",
    "for i in range(0, 7):\n",
    "    print(max_depth_values[i])\n",
    "    forest = RandomForestClassifier(n_estimators=1000,max_depth=max_depth_values[i], random_state=42)\n",
    "    forest.fit(X_train, y_train)\n",
    "    train_accuracy[i] = forest.score(X_train, y_train) \n",
    "    test_accuracy[i] = forest.score(X_test, y_test) \n",
    "    depth_values.append(max_depth_values[i])\n",
    "best_index = np.argmax(test_accuracy)\n",
    "best_alpha = depth_values[best_index]\n",
    "best_test_acc = test_accuracy[best_index]\n",
    "print(f\"robustScaler: Highest Test Accuracy: {best_test_acc:.4f} at depth = {best_alpha}\")\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy') \n",
    "plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy') \n",
    "plt.legend() \n",
    "plt.xlabel('[None,5, 10, 20, 30, 50, 100] max_depth values') \n",
    "plt.ylabel('Accuracy') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection, Feature Importance and Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF Feature selection #1\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "k = 50  # Number of top features to select\n",
    "selector = SelectKBest(score_func=f_classif, k=k)\n",
    "X = selector.fit_transform(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "max_depth_values = [None,5, 10, 20, 30, 50, 100]\n",
    "neighbors = np.arange(1, 8)\n",
    "train_accuracy = np.empty(len(neighbors))\n",
    "test_accuracy = np.empty(len(neighbors))\n",
    "depth_values = []\n",
    "for i in range(0, 7):\n",
    "    print(max_depth_values[i])\n",
    "    forest = RandomForestClassifier(n_estimators=1000,max_depth=max_depth_values[i], random_state=42)\n",
    "    forest.fit(X_train, y_train)\n",
    "    train_accuracy[i] = forest.score(X_train, y_train)\n",
    "    test_accuracy[i] = forest.score(X_test, y_test)\n",
    "    depth_values.append(max_depth_values[i])\n",
    "best_index = np.argmax(test_accuracy)\n",
    "best_alpha = depth_values[best_index]\n",
    "best_test_acc = test_accuracy[best_index]\n",
    "print(f\"Highest Test Accuracy: {best_test_acc:.4f} at depth = {best_alpha}\")\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy')\n",
    "plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('[None,5, 10, 20, 30, 50, 100] max_depth values')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# RF Feature importance graph #1\n",
    "df1 = pd.read_csv(\"Data.csv\")\n",
    "df2 = pd.read_csv(\"extra_hard_samples.csv\")\n",
    "ds = pd.concat([df1, df2], axis=0).reset_index(drop=True)\n",
    "\n",
    "y = ds[\"class\"]\n",
    "images = ds[\"image_name\"]\n",
    "ds = ds.drop(\"class\", axis=1)\n",
    "ds = ds.drop(\"image_name\", axis=1)\n",
    "\n",
    "X=ds.to_numpy()\n",
    "y=y.to_numpy()\n",
    "\n",
    "# RF Feature importance graph #1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "lengthOfFeaturesInOrder = np.arange(1, 257)\n",
    "forest = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "forest.fit(X_train, y_train.ravel())\n",
    "importances = forest.feature_importances_\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.barh(lengthOfFeaturesInOrder, importances, color='skyblue')\n",
    "plt.xlabel('Random Forest Importance')\n",
    "plt.title('Feature Importance - RF Importance')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis for better visualization\n",
    "plt.show()\n",
    "\n",
    "#RF Feature importance graph #2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "RF = RandomForestClassifier(n_estimators=1000, max_depth=None, random_state=0)\n",
    "RF.fit(X_train, y_train.ravel())\n",
    "RF_feature_weights = RF.feature_importances_\n",
    "feature_weights=RF_feature_weights.flatten()\n",
    "# Create a DataFrame to store the features and their corresponding weights\n",
    "features_df = pd.DataFrame({\n",
    "    'Feature': ds.columns,\n",
    "    'Weight': feature_weights\n",
    "})\n",
    "# Sort by the absolute value of weights to see the most important features\n",
    "features_df['Absolute Weight'] = np.abs(features_df['Weight'])\n",
    "features_df = features_df.sort_values(by='Absolute Weight', ascending=False)\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"Feature\", y=\"Weight\", data=features_df.head(20), palette=\"coolwarm\")\n",
    "plt.title(\"Top 10 Features by Weight in RF Model\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Weight\")\n",
    "# Rotate the x labels by 45 degrees\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plotting the RF trees(change n_estimators to 1000 for actual results)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "RF = RandomForestClassifier(n_estimators=1000, max_depth=5, random_state=0)\n",
    "RF.fit(X_train, y_train.ravel())\n",
    "from sklearn import tree\n",
    "plt.figure(figsize=(12, 5))\n",
    "tree.plot_tree(RF.estimators_[0], filled=True)\n",
    "plt.title(\"First Tree in Random Forest\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the last tree\n",
    "plt.figure(figsize=(12, 5))\n",
    "tree.plot_tree(RF.estimators_[-1], filled=True)\n",
    "plt.title(\"Last Tree in Random Forest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBC Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier #4: GBC ----------------------------------------------------------------------------\n",
    "#4-fold validation on n_estimators with max_depth=3 ~ very long running times\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "n_range = [1, 5, 10, 20, 50, 100, 200, 500] #1, 5, 10, 20, 50, 100, 200, 1000\n",
    "train_scores = []\n",
    "val_scores = []\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=50)\n",
    "\n",
    "for N in n_range:\n",
    "    fold_train_scores = []\n",
    "    fold_val_scores = []\n",
    "    print(\"N running now:\", N)\n",
    "    count = 0\n",
    "    for train_index, val_index in kf.split(X_train, y_train):\n",
    "        count += 1\n",
    "        print(\"Fold running now:\", count)\n",
    "        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        gbc = GradientBoostingClassifier(n_estimators=N, max_depth=3, random_state=42)\n",
    "        gbc.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        fold_train_scores.append(gbc.score(X_train_fold, y_train_fold))\n",
    "        fold_val_scores.append(gbc.score(X_val_fold, y_val_fold))\n",
    "\n",
    "    train_scores.append(np.mean(fold_train_scores))\n",
    "    val_scores.append(np.mean(fold_val_scores))\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.plot(n_range, train_scores, marker='o', linestyle='-', color='blue', label='Train Accuracy')\n",
    "plt.plot(n_range, val_scores, marker='o', linestyle='-', color='red', label='Validation Accuracy')\n",
    "plt.xscale('log')  # Use a logarithmic scale for C\n",
    "plt.xlabel('Regularization Strength (C)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Validation Accuracy vs. C')\n",
    "plt.ylim(0.1, 1.1)  # set y-axis limits\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "best_index = np.argmax(val_scores)\n",
    "best_n = n_range[best_index]\n",
    "best_val_acc = val_scores[best_index]\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.4f} for N = {best_n:.4e}\")\n",
    "\n",
    "#4-fold validation on learning_rate with max_depth=3 & n_Estimators=500 ~ very long running times --\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "train_scores = []\n",
    "val_scores = []\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=50)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    fold_train_scores = []\n",
    "    fold_val_scores = []\n",
    "    print(\"learning_rate running now:\", lr)\n",
    "    count = 0\n",
    "    for train_index, val_index in kf.split(X_train, y_train):\n",
    "        count += 1\n",
    "        print(\"Fold running now:\", count)\n",
    "        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        gbc = GradientBoostingClassifier(n_estimators=5,learning_rate=lr, max_depth=3, random_state=42)\n",
    "        gbc.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        fold_train_scores.append(gbc.score(X_train_fold, y_train_fold))\n",
    "        fold_val_scores.append(gbc.score(X_val_fold, y_val_fold))\n",
    "\n",
    "    train_scores.append(np.mean(fold_train_scores))\n",
    "    val_scores.append(np.mean(fold_val_scores))\n",
    "best_index = np.argmax(val_scores)\n",
    "best_n = learning_rates[best_index]\n",
    "best_val_acc = val_scores[best_index]\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.4f} for learning_rate = {best_n:.4e}\")\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.plot(learning_rates, train_scores, marker='o', linestyle='-', color='blue', label='Train Accuracy')\n",
    "plt.plot(learning_rates, val_scores, marker='o', linestyle='-', color='red', label='Validation Accuracy')\n",
    "plt.xscale('log')  # Use a logarithmic scale for C\n",
    "plt.xlabel('learning_rate')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Validation Accuracy vs. learning_rate')\n",
    "plt.ylim(0.1, 1.1)  # set y-axis limits\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GBC: no preprocessing\n",
    "#GBC Data normalization #1: MinMax\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "numToPick = 9 #+1\n",
    "arrayToTestOnAll = [1, 5, 10, 20, 50, 100, 200, 500]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "n_range = arrayToTestOnAll\n",
    "neighbors = np.arange(1, numToPick) \n",
    "train_accuracy = np.empty(len(neighbors)) \n",
    "test_accuracy = np.empty(len(neighbors)) \n",
    "for i in range(0, (numToPick-1)): \n",
    "    print(n_range[i])\n",
    "    gbc = GradientBoostingClassifier(n_estimators=(n_range[i]), max_depth=3, learning_rate=0.1, random_state=42)\n",
    "    gbc.fit(X_train, y_train.ravel())\n",
    "    train_accuracy[i] = gbc.score(X_train, y_train) \n",
    "    test_accuracy[i] = gbc.score(X_test, y_test) \n",
    "best_index = np.argmax(test_accuracy)\n",
    "best_alpha = depth_values[best_index]\n",
    "best_test_acc = test_accuracy[best_index]\n",
    "print(f\"MinMax: Highest Test Accuracy: {best_test_acc:.4f} at depth = {best_alpha}\")\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy') \n",
    "plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy') \n",
    "plt.legend() \n",
    "plt.xlabel('[1, 5, 10, 20, 50, 100, 200, 1000] n_estimator values') \n",
    "plt.ylabel('Accuracy') \n",
    "plt.show()\n",
    "\n",
    "#GBC Data normalization #2: Mean-Sigma\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "if (X < 0).any():\n",
    "    X -= X.min(axis=0)  #make all features non-negative\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "n_range = arrayToTestOnAll\n",
    "neighbors = np.arange(1, numToPick) \n",
    "train_accuracy = np.empty(len(neighbors)) \n",
    "test_accuracy = np.empty(len(neighbors)) \n",
    "for i in range(0, (numToPick-1)): \n",
    "    print(n_range[i])\n",
    "    gbc = GradientBoostingClassifier(n_estimators=(n_range[i]), max_depth=3, learning_rate=0.1, random_state=42)\n",
    "    gbc.fit(X_train, y_train.ravel())\n",
    "    train_accuracy[i] = gbc.score(X_train, y_train) \n",
    "    test_accuracy[i] = gbc.score(X_test, y_test) \n",
    "best_index = np.argmax(test_accuracy)\n",
    "best_alpha = depth_values[best_index]\n",
    "best_test_acc = test_accuracy[best_index]\n",
    "print(f\"Mean-Sigma: Highest Test Accuracy: {best_test_acc:.4f} at depth = {best_alpha}\")\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy') \n",
    "plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy') \n",
    "plt.legend() \n",
    "plt.xlabel('[1, 5, 10, 20, 50, 100, 200, 1000] n_estimator values') \n",
    "plt.ylabel('Accuracy') \n",
    "plt.show()\n",
    "\n",
    "#GBC Data normalization #3: Median-MAD\n",
    "def median_mad_normalize(X): #SOURCE: google generative ai when searched in google \"median mad normalization in python\"\n",
    "    medians = np.median(X, axis=0)\n",
    "    mad = np.median(np.abs(X - medians), axis=0)\n",
    "    mad[mad == 0] = 1e-6\n",
    "    X_normalized = (X - medians) / mad\n",
    "    return X_normalized\n",
    "X = median_mad_normalize(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "n_range = arrayToTestOnAll\n",
    "neighbors = np.arange(1, numToPick) \n",
    "train_accuracy = np.empty(len(neighbors)) \n",
    "test_accuracy = np.empty(len(neighbors)) \n",
    "for i in range(0, (numToPick-1)): \n",
    "    print(n_range[i])\n",
    "    gbc = GradientBoostingClassifier(n_estimators=(n_range[i]), max_depth=3, learning_rate=0.1, random_state=42)\n",
    "    gbc.fit(X_train, y_train.ravel())\n",
    "    train_accuracy[i] = gbc.score(X_train, y_train) \n",
    "    test_accuracy[i] = gbc.score(X_test, y_test) \n",
    "best_index = np.argmax(test_accuracy)\n",
    "best_alpha = depth_values[best_index]\n",
    "best_test_acc = test_accuracy[best_index]\n",
    "print(f\"Median-MAD: Highest Test Accuracy: {best_test_acc:.4f} at depth = {best_alpha}\")\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy') \n",
    "plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy') \n",
    "plt.legend() \n",
    "plt.xlabel('[1, 5, 10, 20, 50, 100, 200, 1000] n_estimator values') \n",
    "plt.ylabel('Accuracy') \n",
    "plt.show()\n",
    "\n",
    "#GBC Data normalization #4: robustScaler\n",
    "robust_scaler = RobustScaler()\n",
    "X = robust_scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "n_range = arrayToTestOnAll\n",
    "neighbors = np.arange(1, numToPick) \n",
    "train_accuracy = np.empty(len(neighbors)) \n",
    "test_accuracy = np.empty(len(neighbors)) \n",
    "for i in range(0, (numToPick-1)): \n",
    "    print(n_range[i])\n",
    "    gbc = GradientBoostingClassifier(n_estimators=(n_range[i]), max_depth=3, learning_rate=0.1, random_state=42)\n",
    "    gbc.fit(X_train, y_train.ravel())\n",
    "    train_accuracy[i] = gbc.score(X_train, y_train) \n",
    "    test_accuracy[i] = gbc.score(X_test, y_test) \n",
    "best_index = np.argmax(test_accuracy)\n",
    "best_alpha = depth_values[best_index]\n",
    "best_test_acc = test_accuracy[best_index]\n",
    "print(f\"robustScaler: Highest Test Accuracy: {best_test_acc:.4f} at depth = {best_alpha}\")\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy') \n",
    "plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy') \n",
    "plt.legend() \n",
    "plt.xlabel('[1, 5, 10, 20, 50, 100, 200, 1000] n_estimator values') \n",
    "plt.ylabel('Accuracy') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection, Feature Importance and Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBC Feature selection #1\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "k = 50  # Number of top features to select\n",
    "selector = SelectKBest(score_func=f_classif, k=k)\n",
    "X = selector.fit_transform(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "n_range = [1, 5, 10, 20, 50, 100, 200, 500]\n",
    "neighbors = np.arange(1, 9)\n",
    "train_accuracy = np.empty(len(neighbors))\n",
    "test_accuracy = np.empty(len(neighbors))\n",
    "for i in range(0, 8):\n",
    "    print(n_range[i])\n",
    "    gbc = GradientBoostingClassifier(n_estimators=(n_range[i]), max_depth=3, learning_rate=0.1, random_state=42)\n",
    "    gbc.fit(X_train, y_train.ravel())\n",
    "    train_accuracy[i] = gbc.score(X_train, y_train)\n",
    "    test_accuracy[i] = gbc.score(X_test, y_test)\n",
    "best_index = np.argmax(test_accuracy)\n",
    "best_alpha = depth_values[best_index]\n",
    "best_test_acc = test_accuracy[best_index]\n",
    "print(f\"GBC Feature selection 1: Highest Test Accuracy: {best_test_acc:.4f} at depth = {best_alpha}\")\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy')\n",
    "plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('[1, 5, 10, 20, 50, 100, 200, 1000] n_estimator values')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# GBC feature selection #2\n",
    "df1 = pd.read_csv(\"Data.csv\")\n",
    "df2 = pd.read_csv(\"extra_hard_samples.csv\")\n",
    "ds = pd.concat([df1, df2], axis=0).reset_index(drop=True)\n",
    "\n",
    "y = ds[\"class\"]\n",
    "images = ds[\"image_name\"]\n",
    "ds = ds.drop(\"class\", axis=1)\n",
    "ds = ds.drop(\"image_name\", axis=1)\n",
    "X=ds.to_numpy()\n",
    "y=y.to_numpy()\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X, y)\n",
    "selector = SelectFromModel(rf, threshold=\"median\") #selects above the median\n",
    "selector.fit(X, y)\n",
    "X_selected = selector.transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "gbc = GradientBoostingClassifier(n_estimators=500, max_depth=3, learning_rate=0.1, random_state=42)\n",
    "gbc.fit(X_train, y_train)\n",
    "y_pred = gbc.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy with RF-selected features: {accuracy:.4f}\")\n",
    "selected_features = selector.get_support(indices=True)\n",
    "print(f\"Selected feature indices: {selected_features}\")\n",
    "\n",
    "# GBC Feature importance #1\n",
    "lengthOfFeaturesInOrder = np.arange(1, 257)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "gbc = GradientBoostingClassifier(n_estimators=500, max_depth=3, learning_rate=0.1, random_state=42)\n",
    "gbc.fit(X_train, y_train)\n",
    "importances = gbc.feature_importances_\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.barh(lengthOfFeaturesInOrder, importances, color='skyblue')\n",
    "plt.xlabel('GradientBoostingClassifier Importance')\n",
    "plt.title('Feature Importance - GBC Importance')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis for better visualization\n",
    "plt.show()\n",
    "\n",
    "# GBC Feature importance #2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "gbc = GradientBoostingClassifier(n_estimators=500, max_depth=3, learning_rate=0.1, random_state=42)\n",
    "gbc.fit(X_train, y_train)\n",
    "RF_feature_weights = gbc.feature_importances_\n",
    "feature_weights=RF_feature_weights.flatten()\n",
    "# Create a DataFrame to store the features and their corresponding weights\n",
    "features_df = pd.DataFrame({\n",
    "    'Feature': ds.columns,\n",
    "    'Weight': feature_weights\n",
    "})\n",
    "# Sort by the absolute value of weights to see the most important features\n",
    "features_df['Absolute Weight'] = np.abs(features_df['Weight'])\n",
    "features_df = features_df.sort_values(by='Absolute Weight', ascending=False)\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"Feature\", y=\"Weight\", data=features_df.head(20), palette=\"coolwarm\")\n",
    "plt.title(\"Top 10 Features by Weight in RF Model\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Weight\")\n",
    "# Rotate the x labels by 45 degrees\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plotting trees from GBC\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "gbc = GradientBoostingClassifier(n_estimators=500, max_depth=3, learning_rate=0.1, random_state=42)\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "tree.plot_tree(gbc.estimators_[0,0], filled=True)\n",
    "plt.title(\"First Tree in GBR\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the last tree\n",
    "plt.figure(figsize=(12, 5))\n",
    "tree.plot_tree(gbc.estimators_[-1,0], filled=True)\n",
    "plt.title(\"Last Tree in GBR\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks which normalization was the best: stand wins - copy and pasted from hw4 solution\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaling_methods = ['No Scaling', 'MinMax', 'Standard']\n",
    "scalers = {\n",
    "    'No Scaling': None,\n",
    "    'MinMax': MinMaxScaler(),\n",
    "    'Standard': StandardScaler()\n",
    "}\n",
    "\n",
    "scaling_method_param = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "\n",
    "# Stratified K-Fold setup\n",
    "skf = StratifiedKFold(n_splits=4)\n",
    "\n",
    "# Iterate over each scaling method\n",
    "for scale_method in scaling_methods:\n",
    "    scaler = scalers[scale_method]\n",
    "\n",
    "    # Apply scaling if scaler is defined, else keep original X_train\n",
    "    if scaler is not None:\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "    else:\n",
    "        X_train_scaled = X_train  # assuming X_train is a DataFrame\n",
    "\n",
    "    print(f\"Scaling: {scale_method}\")\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # Stratified K-Fold\n",
    "    for train_index, val_index in skf.split(X_train_scaled, y_train):\n",
    "        X_tr, X_val = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "        y_tr, y_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        # SVM model with default parameters\n",
    "        model = SVC(kernel='rbf')  # Default SVM parameters\n",
    "        model.fit(X_tr, y_tr)\n",
    "\n",
    "        # Store accuracies\n",
    "        train_accuracies.append(model.score(X_tr, y_tr))\n",
    "        val_accuracies.append(model.score(X_val, y_val))\n",
    "\n",
    "    # Calculate average accuracies\n",
    "    avg_train_accuracy = np.mean(train_accuracies)\n",
    "    avg_val_accuracy = np.mean(val_accuracies)\n",
    "    print(f\"Avg Val Acc for Scaling={scale_method}: {avg_val_accuracy}\")\n",
    "\n",
    "    # Store results\n",
    "    scaling_method_param.append(scale_method)\n",
    "    train_acc.append(avg_train_accuracy)\n",
    "    val_acc.append(avg_val_accuracy)\n",
    "\n",
    "# Create DataFrame for results\n",
    "results_df = pd.DataFrame({\n",
    "    'Scaling Method': scaling_method_param,\n",
    "    'Train Accuracy': train_acc,\n",
    "    'Validation Accuracy': val_acc\n",
    "})\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X=ds.to_numpy()\n",
    "# y=y.to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "C_values = [0.1, 10, 100]\n",
    "gamma_values = [0.01, 0.1, 1]\n",
    "\n",
    "C_param = []\n",
    "gamma_param = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=4)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "for C in C_values:\n",
    "    for gamma in gamma_values:\n",
    "        print(f\"C = {C}, Gamma = {gamma}\")\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "\n",
    "        for train_index, val_index in skf.split(X_train_scaled, y_train):\n",
    "            X_tr, X_val = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "            y_tr, y_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "            model = SVC(kernel='rbf', C=C, gamma=gamma)\n",
    "\n",
    "            model.fit(X_tr, y_tr)\n",
    "\n",
    "            train_accuracy = model.score(X_tr, y_tr)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "\n",
    "            val_accuracy = model.score(X_val, y_val)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "\n",
    "        avg_train_accuracy = np.mean(train_accuracies)\n",
    "        avg_val_accuracy = np.mean(val_accuracies)\n",
    "        print(f\"Avg Val Acc for C={C}, Gamma={gamma}: {avg_val_accuracy}\")\n",
    "\n",
    "        # Store results\n",
    "        C_param.append(C)\n",
    "        gamma_param.append(gamma)\n",
    "        train_acc.append(avg_train_accuracy)\n",
    "        val_acc.append(avg_val_accuracy)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'C': C_param,\n",
    "    'Gamma': gamma_param,\n",
    "    'Train Accuracy': train_acc,\n",
    "    'Validation Accuracy': val_acc\n",
    "})\n",
    "# Plot training and validation accuracies in the same plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Loop through gamma values and plot both training and validation accuracy for each\n",
    "for gamma in gamma_values:\n",
    "    subset = results_df[results_df['Gamma'] == gamma]\n",
    "    plt.plot(subset['C'], subset['Train Accuracy'], label=f'Training Accuracy (Gamma={gamma})', marker='o')\n",
    "    plt.plot(subset['C'], subset['Validation Accuracy'], label=f'Validation Accuracy (Gamma={gamma})', marker='o', linestyle='--')\n",
    "\n",
    "max_val_index = np.argmax(val_acc)\n",
    "\n",
    "# Retrieve the corresponding C, Gamma, and Validation Accuracy\n",
    "best_C = C_param[max_val_index]\n",
    "best_gamma = gamma_param[max_val_index]\n",
    "best_val_acc = val_acc[max_val_index]\n",
    "\n",
    "# Print the results\n",
    "print(f\"Highest Validation Accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"Best Parameters: C = {best_C}, Gamma = {best_gamma}\")\n",
    "\n",
    "# Set x-axis to logarithmic scale for C\n",
    "plt.xscale('log')\n",
    "\n",
    "# Labels, title, and legend\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy for Different C and Gamma Values')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "C_values = [0.1, 10, 100]\n",
    "gamma_values = [0.01, 0.1, 1]\n",
    "degree_values = [2, 3, 4]\n",
    "\n",
    "C_param = []\n",
    "gamma_param = []\n",
    "degree_param = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=4)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "for C in C_values:\n",
    "    for gamma in gamma_values:\n",
    "        for degree in degree_values:\n",
    "            print(f\"C = {C}, Gamma = {gamma}, Degree = {degree}\")\n",
    "            train_accuracies = []\n",
    "            val_accuracies = []\n",
    "\n",
    "            for train_index, val_index in skf.split(X_train_scaled, y_train):\n",
    "                X_tr, X_val = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "                y_tr, y_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "                #polynomial kernel SVM model\n",
    "                model = SVC(kernel='poly', C=C, gamma=gamma, degree=degree)\n",
    "\n",
    "\n",
    "                model.fit(X_tr, y_tr)\n",
    "\n",
    "                # Accuracy on training set\n",
    "                train_accuracy = model.score(X_tr, y_tr)\n",
    "                train_accuracies.append(train_accuracy)\n",
    "\n",
    "                # Accuracy on validation set\n",
    "                val_accuracy = model.score(X_val, y_val)\n",
    "                val_accuracies.append(val_accuracy)\n",
    "\n",
    "            avg_train_accuracy = np.mean(train_accuracies)\n",
    "            avg_val_accuracy = np.mean(val_accuracies)\n",
    "            print(f\"Avg Val Acc for C={C}, Gamma={gamma}, Degree={degree}: {avg_val_accuracy}\")\n",
    "\n",
    "            C_param.append(C)\n",
    "            gamma_param.append(gamma)\n",
    "            degree_param.append(degree)\n",
    "            train_acc.append(avg_train_accuracy)\n",
    "            val_acc.append(avg_val_accuracy)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'C': C_param,\n",
    "    'Gamma': gamma_param,\n",
    "    'Degree': degree_param,\n",
    "    'Train Accuracy': train_acc,\n",
    "    'Validation Accuracy': val_acc\n",
    "})\n",
    "\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Section Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "svm = SVC(kernel='rbf', probability=True, C=100)\n",
    "svm.fit(X_train, y_train.ravel())\n",
    "\n",
    "y_probMLP = svm.predict_proba(X_test)\n",
    "num_classes = y_probMLP.shape[1]\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(num_classes):\n",
    "    plt.subplot(1, num_classes, i + 1)\n",
    "    plt.scatter(y_test, y_probMLP[:, i], alpha=0.5)\n",
    "    plt.title(f'Class {i} Probability')\n",
    "    plt.xlabel('True Labels')\n",
    "    plt.ylabel('Predicted Probability')\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Algorithm with Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier #6: MLP --------------------------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaling_methods = ['No Scaling', 'MinMax', 'Standard']\n",
    "scalers = {\n",
    "    'No Scaling': None,\n",
    "    'MinMax': MinMaxScaler(),\n",
    "    'Standard': StandardScaler()\n",
    "}\n",
    "\n",
    "scaling_method_param = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "\n",
    "# Stratified K-Fold setup\n",
    "skf = StratifiedKFold(n_splits=4)\n",
    "\n",
    "# Iterate over each scaling method\n",
    "for scale_method in scaling_methods:\n",
    "    scaler = scalers[scale_method]\n",
    "\n",
    "    # Apply scaling if scaler is defined, else keep original X_train\n",
    "    if scaler is not None:\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "    else:\n",
    "        X_train_scaled = X_train  # assuming X_train is a DataFrame\n",
    "\n",
    "    # Initialize lists to store accuracies for each fold\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    print(f\"Scaling Method: {scale_method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layer Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_layer_szie parameter check ----------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "layer_configs = [(1,), (3,),(5,)]\n",
    "node_counts = [64, 128, 256]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Set up 4-fold Stratified Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=4)\n",
    "\n",
    "# Initialize lists to store results\n",
    "layer_param = []\n",
    "node_param = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "\n",
    "# Loop through each combination of layer count and node count\n",
    "for layers in layer_configs:\n",
    "    for nodes in node_counts:\n",
    "        print(f\"Layers = {layers[0]}, Nodes = {nodes}\")\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "\n",
    "        # Create hidden layer configuration\n",
    "        hidden_layer_sizes = tuple([nodes] * layers[0])\n",
    "\n",
    "        # Perform 4-fold Stratified cross-validation\n",
    "        for train_index, val_index in skf.split(X_train_scaled, y_train):\n",
    "            X_tr, X_val = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "            y_tr, y_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "            # Define the MLP model\n",
    "            model = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, max_iter=300, random_state=42)\n",
    "\n",
    "            # Train the model\n",
    "            model.fit(X_tr, y_tr)\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            train_accuracy = model.score(X_tr, y_tr)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "\n",
    "            # Calculate validation accuracy\n",
    "            val_accuracy = model.score(X_val, y_val)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "\n",
    "        # Average training and validation accuracies across the 4 cases\n",
    "        avg_train_accuracy = np.mean(train_accuracies)\n",
    "        avg_val_accuracy = np.mean(val_accuracies)\n",
    "        print(avg_val_accuracy)\n",
    "        # Store results for plotting\n",
    "        layer_param.append(layers[0])\n",
    "        node_param.append(nodes)\n",
    "        train_acc.append(avg_train_accuracy)\n",
    "        val_acc.append(avg_val_accuracy)\n",
    "\n",
    "# Convert results to a DataFrame for easier plotting\n",
    "results_df = pd.DataFrame({\n",
    "    'Layers': layer_param,\n",
    "    'Nodes': node_param,\n",
    "    'Train Accuracy': train_acc,\n",
    "    'Validation Accuracy': val_acc\n",
    "})\n",
    "\n",
    "# Plot training and validation accuracies\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot for training accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "for nodes in node_counts:\n",
    "    subset = results_df[results_df['Nodes'] == nodes]\n",
    "    plt.plot(subset['Layers'], subset['Train Accuracy'], label=f'Nodes={nodes}', marker='o')\n",
    "plt.xlabel('Number of Layers')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot for validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "for nodes in node_counts:\n",
    "    subset = results_df[results_df['Nodes'] == nodes]\n",
    "    plt.plot(subset['Layers'], subset['Validation Accuracy'], label=f'Nodes={nodes}', marker='o')\n",
    "plt.xlabel('Number of Layers')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.suptitle('Training and Validation Accuracy for Different MLP Configurations')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP: other parameters ----------------------------------------------------------------------------\n",
    "mainRange = [.1, 1, 10, 100]\n",
    "train_scores = []\n",
    "val_scores = []\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=50)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "solverStrings = ['lbfgs', 'lbfgs', 'adam', 'adam']\n",
    "hiddenLaySize = [[10,10],[10],[10,10],[10]]\n",
    "\n",
    "for A in mainRange:\n",
    "    fold_train_scores = []\n",
    "    fold_val_scores = []\n",
    "    print(\"A running now:\", A)\n",
    "    count = 0\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        count += 1\n",
    "        print(\"Fold running now:\", count)\n",
    "        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        mlp = MLPClassifier(solver=solverStrings[(count-1)], random_state=0, alpha=A, hidden_layer_sizes=hiddenLaySize[(count-1)])\n",
    "        mlp.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        print(\"Validation Score: \" + str(mlp.score(X_val_fold, y_val_fold)) + \". solver=\" + str(solverStrings[(count-1)]) + \". alpha=\" + str(A) + \". hidden_layer_sizes=\" + str(hiddenLaySize[(count-1)]))\n",
    "        fold_train_scores.append(mlp.score(X_train_fold, y_train_fold))\n",
    "        fold_val_scores.append(mlp.score(X_val_fold, y_val_fold))\n",
    "\n",
    "    train_scores.append(np.mean(fold_train_scores))\n",
    "    val_scores.append(np.mean(fold_val_scores))\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.plot(mainRange, train_scores, marker='o', linestyle='-', color='blue', label='Train Accuracy')\n",
    "plt.plot(mainRange, val_scores, marker='o', linestyle='-', color='red', label='Validation Accuracy')\n",
    "plt.xscale('log')  # Use a logarithmic scale for C\n",
    "plt.xlabel('Regularization Strength (C)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Validation Accuracy vs. C')\n",
    "plt.ylim(0.1, 1.1)  # set y-axis limits\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "best_index = np.argmax(val_scores)\n",
    "best_alpha = mainRange[best_index]\n",
    "best_val_acc = val_scores[best_index]\n",
    "print(\"best value:\", best_val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Predicting Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP predicting pobability ----------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "mlp = MLPClassifier(solver='lbfgs', random_state=0, alpha=100, hidden_layer_sizes=[10])\n",
    "mlp.fit(X_train, y_train.ravel())\n",
    "\n",
    "y_probMLP = mlp.predict_proba(X_test)\n",
    "num_classes = y_probMLP.shape[1]\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(num_classes):\n",
    "    plt.subplot(1, num_classes, i + 1)\n",
    "    plt.scatter(y_test, y_probMLP[:, i], alpha=0.5)\n",
    "    plt.title(f'Class {i} Probability')\n",
    "    plt.xlabel('True Labels')\n",
    "    plt.ylabel('Predicted Probability')\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM and MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC and MLP together:  --------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "Xtrain = scaler.fit_transform(X_train)\n",
    "Xtest = scaler.transform(X_test)\n",
    "\n",
    "mlp = MLPClassifier(solver='lbfgs', random_state=0, alpha=100, hidden_layer_sizes=[10])\n",
    "mlp.fit(Xtrain, y_train.ravel())\n",
    "\n",
    "svm = SVC(kernel='rbf', probability=True, C=100)\n",
    "svm.fit(Xtrain, y_train.ravel())\n",
    "\n",
    "proba1 = mlp.predict_proba(Xtest)[:, 1]\n",
    "proba2 = svm.predict_proba(Xtest)[:, 1]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_test_numeric = label_encoder.fit_transform(y_test)\n",
    "\n",
    "plt.scatter(proba1, proba2, c=y_test_numeric, cmap='viridis')\n",
    "plt.xlabel(\"Model 1 Probability\")\n",
    "plt.ylabel(\"Model 2 Probability\")\n",
    "plt.title(\"Scatter Plot of Predicted Probabilities\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "svm_probs = svm.predict_proba(Xtest)\n",
    "mlp_probs = mlp.predict_proba(Xtest)\n",
    "\n",
    "# Scatter plots for each class\n",
    "class_labels = {0: 'person', 1: 'sign', 2: 'bike', 3: 'bus', 4: 'car'}\n",
    "n_classes = len(class_labels)\n",
    "fig, axes = plt.subplots(1, n_classes, figsize=(15, 4))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.scatter(svm_probs[:, i], mlp_probs[:, i], alpha=0.5)\n",
    "    ax.set_title(f'Class {class_labels[i]}')\n",
    "    ax.set_xlabel('SVM Probability')\n",
    "    ax.set_ylabel('MLP Probability')\n",
    "\n",
    "plt.suptitle('SVM vs. MLP Probability Scatter Plots per Class')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods: Bagging and AdaBoost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finalproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
